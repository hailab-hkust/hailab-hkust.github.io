<!DOCTYPE html>
<html lang="en" class="fontawesome-i2svg-active fontawesome-i2svg-complete"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">        
<title>Zhiming Hu - Homepage</title>        
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" media="all" href="./index/css/main_v2.css">
<link rel="stylesheet" media="all" href="./index/css/owl.carousel.min.css">
<link rel="stylesheet" media="all" href="./index/css/owl.theme.default.min.css">
<link rel="stylesheet" media="all" href="./index/css/jquery-ui.min.css">
<link rel="stylesheet" href="./index/css/swipe.css">
<script src="./index/js/jquery-3.4.1.min.js"></script>
<script src="./index/js/jquery-ui.min.js"></script>
<script src="./index/js/fontawesome-5.11.2.js"></script>
<script src="./index/js/main.js"></script>
<script src="./index/js/owl.carousel.min.js"></script>
<script src="./index/js/rot13.js"></script>
<script src="./index/js/script.js"></script>
<script src="https://www.w3counter.com/tracker.js?id=150008"></script>
</head>

<body class="header header-location">
<div class="wrapper">
<!-- header-->
<div id="unstickyheader" class="unstickyheader">
<div class="topbar">
<div class="container">
<div class="header-image-container">
	<!--<a href=""><img class="header-image" src="./index/" alt=""></a>-->
</div>
<div class="header-image-container-right">
	<!--a href=""><img class="header-image" src="./index/image/" alt=""></a>-->
</div>
</div>
</div>
</div>
</div>


<!-- content -->
<div class="content">
<div class="section margin-top-20 margin-bottom-20">
<div class="container">

<div class="person">
  <img class="personpic" title="Zhiming Hu" src="./index/image/hu.png" alt="Zhiming Hu">

  
<div class="personinfo">
<h3>Zhiming Hu</h3>

<p><a class="a-text-ext" href="https://www.hkust-gz.edu.cn/" target="_blank" title="The Hong Kong University of Science and Technology (Guangzhou)" rel="nofollow">The Hong Kong University of Science and Technology (Guangzhou)</a></p>
<p>
	<svg class="svg-inline--fa fa-envelope fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="envelope" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"></path></svg><!-- <i class="fa fa-envelope"></i> --><span>zhiminghu (at) hkust-gz.edu.cn</span>
	<svg class="svg-inline--fa fa-map-marker-alt fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="map-marker-alt" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M172.268 501.67C26.97 291.031 0 269.413 0 192 0 85.961 85.961 0 192 0s192 85.961 192 192c0 77.413-26.97 99.031-172.268 309.67-9.535 13.774-29.93 13.773-39.464 0zM192 272c44.183 0 80-35.817 80-80s-35.817-80-80-80-80 35.817-80 80 35.817 80 80 80z"></path></svg><!-- <i class="fa fa-map-marker-alt"></i> --><span>No.1 Duxue Road, Nansha District, Guangzhou, 511453, China</span>
	<svg class="svg-inline--fa fa-door-open fa-w-20" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="door-open" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" data-fa-i2svg=""><path fill="currentColor" d="M624 448h-80V113.45C544 86.19 522.47 64 496 64H384v64h96v384h144c8.84 0 16-7.16 16-16v-32c0-8.84-7.16-16-16-16zM312.24 1.01l-192 49.74C105.99 54.44 96 67.7 96 82.92V448H16c-8.84 0-16 7.16-16 16v32c0 8.84 7.16 16 16 16h336V33.18c0-21.58-19.56-37.41-39.76-32.17zM264 288c-13.25 0-24-14.33-24-32s10.75-32 24-32 24 14.33 24 32-10.75 32-24 32z"></path></svg><!-- <i class="fa fa-door-open"></i> --><span>Building W4, Room Number 406</span>
	<svg class="svg-inline--fa fa-graduation-cap fa-w-20" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="graduation-cap" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" data-fa-i2svg=""><path fill="currentColor" d="M622.34 153.2L343.4 67.5c-15.2-4.67-31.6-4.67-46.79 0L17.66 153.2c-23.54 7.23-23.54 38.36 0 45.59l48.63 14.94c-10.67 13.19-17.23 29.28-17.88 46.9C38.78 266.15 32 276.11 32 288c0 10.78 5.68 19.85 13.86 25.65L20.33 428.53C18.11 438.52 25.71 448 35.94 448h56.11c10.24 0 17.84-9.48 15.62-19.47L82.14 313.65C90.32 307.85 96 298.78 96 288c0-11.57-6.47-21.25-15.66-26.87.76-15.02 8.44-28.3 20.69-36.72L296.6 284.5c9.06 2.78 26.44 6.25 46.79 0l278.95-85.7c23.55-7.24 23.55-38.36 0-45.6zM352.79 315.09c-28.53 8.76-52.84 3.92-65.59 0l-145.02-44.55L128 384c0 35.35 85.96 64 192 64s192-28.65 192-64l-14.18-113.47-145.03 44.56z"></path></svg><!-- <i class="fa fa-graduation-cap"></i> --><span><a class="a-text-ext" href="https://scholar.google.com/citations?user=OLB_xBEAAAAJ" title="google scholar">google scholar</a>, <a class="a-text-ext" href="https://github.com/cranehzm" title="github">github</a>, <a class="a-text-ext" href="./curriculum vitae/curriculum_vitae_English.pdf" title="curriculum vitae">curriculum vitae (English)</a>, <a class="a-text-ext" href="./curriculum vitae/curriculum_vitae_Chinese.pdf" title="curriculum vitae">curriculum vitae (Chinese)</a><br>	
	<a class="a-text-ext" href="https://mp.weixin.qq.com/s/Hwq7Llhn2hTs-xs2ubfEtQ" title="wechat official account">wechat official account (lab)</a>, <a class="a-text-ext" href="https://mp.weixin.qq.com/s/MAdWdQPBj_--gMZVjR0qOg" title="wechat official account">wechat official account (personal)</a>, <a class="a-text-ext" href="https://www.linkedin.com/in/zhiming-hu-42aa52299/" title="LinkedIn">LinkedIn</a></span>
</p>
</div>
</div>
<div class="clearfix"></div>

<hr>
<h3>Short Bio</h3>
<p>Zhiming Hu is a tenure-track Assistant Professor at <b>The Hong Kong University of Science and Technology (Guangzhou)</b> leading the <a class="a-text-ext" href="https://zhiminghu.net/" title="">Human-centered Artificial Intelligence (HAI) Lab</a> starting from August 2025. He was a post-doctoral researcher in the <a class="a-text-ext" href="https://www.perceptualui.org/" title="">Collaborative Artificial Intelligence Lab</a> led by <a class="a-text-ext" href="https://www.perceptualui.org/people/bulling/" title="">Prof. Andreas Bulling</a> and the <a class="a-text-ext" href="https://www.imsb.uni-stuttgart.de/" title="">Computational Biophysics and Biorobotics Lab</a> led by <a class="a-text-ext" href="https://www.imsb.uni-stuttgart.de/team/Schmitt-00006/" title="">Prof. Syn Schmitt</a>, in the <b>University of Stuttgart</b>, Germany from August 2022 to July 2025.
He obtained his Ph.D. degree in Computer Software and Theory from <b>Peking University</b>, China in 2022, supervised by <a class="a-text-ext" href="https://www.graphics.pku.edu.cn/xztd/jgfaculty/wgp2/index.htm#EN_intro" title="">Prof. Guoping Wang</a>. He received his Bachelor's degree in Optical Engineering from <b>Beijing Institute of Technology</b>, China in 2017. His research interests include virtual and augmented reality, human-computer interaction, eye tracking, embodied AI, and human-centered AI. He has published over 20 papers at top venues in VR/AR and HCI, including SIGGRAPH, TVCG, IEEE VR, ISMAR, CHI, and UIST. His work has won <b>Best Journal Paper Award at ISMAR 2024</b> (the only one at the conference), <b>Best Journal Paper Nominees at IEEE VR 2021</b> (first time for Chinese researchers), and <b>Best Student Paper Nominees at INTERACT 2023</b>. He serves as a reviewer for many top venues, including SIGGRAPH, TVCG, IEEE VR, ISMAR, CHI, UIST, IMWUT, CVPR, ICCV, ECCV, AAAI, TMM, and IJHCI.
</p>


<hr>
<h3>Research Interests</h3>
<p>My research interests include virtual and augmented reality, human-computer interaction, eye tracking, embodied AI, and human-centered AI. I mainly work on human behavior analysis and modeling for interactive systems with the purpose of understanding human behavior patterns and building human-centered intelligent interactive systems.
</p>


<hr>
<h3>Recruiting</h3>
<p><font color="red">I am looking for outstanding PostDocs, excellent Ph.D./Master/Intern students, and brilliant research assistans that have an interest in our research.</font></p>

<p>A strong interest in developing human-centered computational methods is required. Excellent programming skills are expected. Previous experience with Python, Pytorch, or CUDA is an advantage. Strong team working and critical thinking skills, aptitude for independent and creative work, as well as fluent English written and presentation skills are essential.</p>

<p>If you are highly motivated and capable of addressing and solving scientifically difficult problems and if you are interested in doing research in an internationally oriented and highly energetic team, you should send your application to zhiminghu (at) hkust-gz.edu.cn.</p>

Please include the following information in your application (preferably in a single pdf document):
<ul>
<li>Curriculum Vitae</li>
<li>Cover letter (stating why you are interested and why you should be chosen)</li>
<li>PhD/Master/Intern applicants: transcripts of master/bachelor program</li>
</ul>
Please note that due to a large amount of applications, I am unable to provide timely replies to all of them. If you don't receive response within two weeks, you are encouraged to contact my colleagues at HKUST (GZ) and I would be willing to co-supervise PhD students with them if there is a fit.


<hr>
<h3>Latest News</h3>
<ul>
<li>[08.2025] <b>I officially started my tenure-track Assistant Professor position at HKUST (GZ).</b></li>
<li>[07.2025] <b>One paper is accepted at UIST 2025.</b></li>
<li>[07.2025] I will serve as a Program Committee for AAAI 2026.</li>
<li>[07.2025] I am invited to give a talk at GAMES Webinar 2025.</li>
<li>[05.2025] <b>One paper is accepted at TVCG 2025.</b></li>
<li>[04.2025] <b>One paper is accepted at SIGGRAPH 2025.</b></li>
<li>[01.2025] <b>One paper is accepted at CHI 2025.</b></li>
<li>[10.2024] <b>Our HOIMotion paper won Best Journal Paper Award at ISMAR 2024!</b></li>
<li>[10.2024] Our GazeMotion paper is invited to present at IROS 2024 workshop on Nonverbal Cues for Human-Robot Cooperative Intelligence.</li>
<li>[10.2024] Our Pose2Gaze paper is invited to present at ISMAR 2024.</li>
<li>[09.2024] <b>I will join the Hong Kong University of Science and Technology (Guangzhou) as a tenure-track Assistant Professor!</b></li>
<li>[08.2024] <b>One paper is accepted at PG 2024.</b></li>
<li>[08.2024] I will serve as a Program Committee for AAAI 2025.</li>
<li>[08.2024] I will serve as the Presentation and Poster Chair for ETRA 2025.</li>
<li>[08.2024] <b>One paper is accepted at UIST 2024.</b></li>
<li>[07.2024] <b>One paper is accepted at ISMAR 2024 journal-track.</b></li>
<li>[06.2024] <b>One paper is accepted at IROS 2024 as oral presentation. </b></li>
<li>[05.2024] I will serve as an Associate Chair for MuC 2024.</li>
<li>[05.2024] <b>One paper is accepted at TVCG 2024.</b></li>
<li>[03.2024] <b>Two papers are accepted at ETRA 2024.</b></li>
<li>[01.2024] <b>Two papers are accepted at CHI 2024.</b></li>
<li>[12.2023] I am invited to attend the 11th Chengyao Youth Forum organised by Nanjing University.</li>
<li>[12.2023] I am invited to be an international program committee for PETMEI 2024. </li>
<li>[12.2023] I am invited to attend the Fifth Youth Forum on the Next Generation Computer Sciences organised by Peking University. </li>
<li>[11.2023] I am invited to attend the 10th Teli Forum organised by Beijing Institute of Technology. </li>
<li>[09.2023] <b>Our paper is nominated for Best Student Paper at INTERACT 2023.</b></li>
<li>[08.2023] I will serve as the Virtualization Chair for ETRA 2024.</li>
<li>[08.2023] <b>One paper is accepted at UIST 2023.</b></li>
<li>[06.2023] I will serve as an Associate Chair for MuC 2023.</li>
<li>[06.2023] One paper is accepted at INTERACT 2023.</li>
<li>[05.2023] I will serve as a Technical Program Committee member for iWOAR 2023.</li>
<li>[10.2022] One paper is accepted at NeurIPS 2022 Workshop Gaze Meets ML.</li>
<li>[08.2022] I joined the University of Stuttgart as a post-doctoral researcher.</li>
<li>[07.2022] <b>I successfully defended my PhD!</b></li>
</ul>


<hr>
<h3>Awards & Honours</h3>
<ul>
<li> <b>Best Journal Paper Award at ISMAR 2024</b> (the only one at the conference)</li>
<li> Baden-Wurttemberg Foundation Postdoctoral Fellowship, 2024</li>
<li> <b>Best Student Paper Nominees at INTERACT 2023</b></li>
<li> SimTech Postdoctoral Fellowship, 2022</li>
<!--<li>Outstanding Graduates Prize in Peking University, 2022</li>-->
<li> National Scholarship (top 2%), 2021</li>
<!--<li> Merit Student Prize in Peking University, 2021</li>-->
<li> <b>Best Journal Paper Nominees at IEEE VR 2021</b> (first time for Chinese researchers)</li>
<li> CSC (China Scholarship Council) Scholarship, 2020</li>
<!--<li> Second-Class Scholarship of Peking University, 2020</li>-->
<!--<li> Merit Student Prize in Peking University, 2020</li>-->
<li> Chancellor's Scholarship (top 2%), 2020</li>
<li> Leo KoGuan Scholarship (top 5%), 2019</li>
<!--<li> Hongcai Scholarship, 2019</li>-->
<!--<li> Merit Student Prize in Peking University, 2019</li>-->
<!--<li> Hongcai Scholarship, 2017</li>-->
<li> Leader Scholarship (top 0.2%, 7 out of over 3800 students), 2017</li>
<!--<li> Outstanding Graduates Prize in Beijing Institute of Technology, 2017</li>-->
<li> National Scholarship (top 2%), 2016</li>
<!--<li> Honorable Mention of Mathematical Contest in Modeling, 2016</li>-->
<!--<li> Merit Student Prize in Beijing Institute of Technology, 2016</li>-->
<!--<li> National Encouragement Scholarship, 2015</li>-->
<!--<li> Merit Student Prize in Beijing Institute of Technology, 2015</li>-->
<!--<li> Third Prize of the National College Students Composition Competition, 2015</li>-->
<!--<li> Pacemaker to Merit Student Prize in Beijing Institute of Technology, 2014</li>-->
<!--<li> Third Prize of Beijing Humanistic Knowledge Competition, 2014</li>-->
<li> National Scholarship (top 2%), 2014</li>
</ul>


<hr>
<h3>Professional Activities & Talks</h3>
<b>Reviewing</b>
<ul>
<li> Journals: TVCG, IMWUT, TMM, IJHCI, TiiS, MTAP, VR, BRM</li>
<li> Conferences: SIGGRAPH, CVPR, ICCV, ECCV, CHI, UIST, IEEE VR, ISMAR, AAAI, PG, ETRA</li>
</ul>

<b>Organising Committee</b>
<ul>
<li> Program Committee for AAAI 2026</li>
<li> Presentation and Poster Chair for ETRA 2025</li>
<li> Program Committee for AAAI 2025</li>
<li> Associate Chair for MuC 2024</li>
<li> International Program Committee for PETMEI 2024</li>
<li> Virtualization Chair for ETRA 2024</li>
<li> Associate Chair for MuC 2023</li>
<li> Technical Program Committee member for iWOAR 2023</li>
</ul>


<b>Invited Talks</b>
<ul>
<li> <a class="a-text-ext" href="./talks/Investigating_the_Coordination_of_Human_Eye_Gaze_and_Body_Movements_in_Extended_Reality.pdf" title="">Investigating the Coordination of Human Eye Gaze and Body Movements in Extended Reality.</a> GAMES Webinar 2025, Hosted by <a class="a-text-ext" href="https://ist.nwu.edu.cn/info/1019/3018.htm" title="">Dr. Xinda Liu</a>, July, 2025.</li>
<li> <a class="a-text-ext" href="./talks/pose2gaze.pdf" title="">Eye-body Coordination during Daily Activities for Gaze Prediction from Full-body Poses.</a> ISMAR 2024, October, 2024.</li>
<li> <a class="a-text-ext" href="./talks/GazeMotion.pdf" title="">Gaze-guided Human Motion Forecasting.</a> IROS 2024 workshop on Nonverbal Cues for Human-Robot Cooperative Intelligence, Hosted by <a class="a-text-ext" href="https://www.jp.honda-ri.com/en/members/chew-jouh-yeong/" title="">Dr. Jouh Yeong Chew</a>, October, 2024.</li>
<li> <a class="a-text-ext" href="./talks/Towards_Human-centered_Artificial_Intelligence.pdf" title="">Towards Human-centered Artificial Intelligence.</a> Nanjing University 11th Chengyao Youth Forum, China, December, 2023.</li>
<li> <a class="a-text-ext" href="./talks/Towards_Human-aware_Intelligent_User_Interfaces.pdf" title="">Towards Human-aware Intelligent User Interfaces.</a> Peking University Fifth Youth Forum on the Next Generation Computer Sciences, China, December, 2023.</li>
<li> <a class="a-text-ext" href="./talks/Towards_the_Coordination_of_Eye_Body_and_Context_in_Daily_Activities.pdf" title="">Towards the Coordination of Eye, Body and Context in Daily Activities.</a> Beijing Institute of Technology 10th Teli Forum, China, Hosted by <a class="a-text-ext" href="https://cs.bit.edu.cn/szdw/jsml/gjjgccrc/wgr_9222aafdaa7c4daf94463e4136277e5b/index.htm" title="">Prof. Guoren Wang</a>, November, 2023.</li>
<li> <a class="a-text-ext" href="./talks/The_Coordination_of_Digital_Humans.pdf" title="">The Coordination of Digital Humans.</a> Peking University Career Talk on Computer Science, China, November, 2022.</li>
<li> <a class="a-text-ext" href="./talks/Analysis_and_Prediction_of_Human_Visual_Attention_in_Virtual_Reality.pdf" title="">Analysis and Prediction of Human Visual Attention in Virtual Reality.</a> Southeast University, China, Hosted by <a class="a-text-ext" href="https://dingdingseu.mystrikingly.com/" title="">Prof. Ding Ding</a>, June, 2022.</li>
<li> <a class="a-text-ext" href="./talks/Recognizing_User_Tasks_from_Eye_and_Head_Movements_in_Immersive_Virtual_Reality.pdf" title="">Recognizing User Tasks from Eye and Head Movements in Immersive Virtual Reality.</a> IEEE VR 2022, Hosted by <a class="a-text-ext" href="https://carelab.info/en/kiyoshi-kiyokawa/" title="">Prof. Kiyoshi Kiyokawa</a>, March, 2022.</li>
<li> <a class="a-text-ext" href="./talks/Forecasting_Eye_Fixations_in_Task-Oriented_Virtual_Environments.pdf" title="">Forecasting Eye Fixations in Task-Oriented Virtual Environments.</a> GAMES Webinar 2021, Hosted by <a class="a-text-ext" href="http://dalab.se.sjtu.edu.cn/www/home/?page_id=143" title="">Prof. Xubo Yang</a>, September, 2021.</li>
<li> <a class="a-text-ext" href="./talks/Gaze_Analysis_and_Prediction_in_Virtual_Reality.pdf" title="">Gaze Analysis and Prediction in Virtual Reality.</a> ChinaVR 2020 - IEEE VR Night, Hosted by <a class="a-text-ext" href="https://scse.buaa.edu.cn/info/1078/2672.htm" title="">Prof. Lili Wang</a>, September 2020.</li>
<li> <a class="a-text-ext" href="./talks/Eye-Head_Coordination_Model_for_Real-time_Gaze_Prediction.pdf" title="">Eye-Head Coordination Model for Real-time Gaze Prediction.</a> 2019 International Conference on VR/AR and 3D Display, Hosted by <a class="a-text-ext" href="http://xufeng.site/" title="">Prof. Feng Xu</a>, June 2019.</li>
</ul>


<hr>
<h3>Teaching</h3>
<ul>
<li> Machine Perception and Learning, University of Stuttgart, 2022, Lecturer</li>
<li> Computer Graphics, Peking University, 2018, Teaching Assistant</li>
<li> Image and Video-Based 3D Reconstruction, Peking University, 2018, Teaching Assistant</li>
<li> Programming Basics, Peking University, 2018, Teaching Assistant</li>
</ul>


<hr>
<h3>Selected Publications</h3>
* Corresponding author  # Equal contribution

<br><br><h4>Journal Papers</h4>
<!--paper-->
<ol class="bibliography"><li>
<div id="hu25_haheae" class="margin-bottom-30">
<a href="./hu25_haheae.html">
<img class="thumbnail" src="./hu25_haheae/image/thumb.png" title="HaHeAE: Learning Generalisable Joint
Representations of Human Hand and Head Movements in Extended Reality" alt="HaHeAE: Learning Generalisable Joint Representations of Human Hand and Head Movements in Extended Reality">
</a>
<p class="pub_title">HaHeAE: Learning Generalisable Joint Representations of Human Hand and Head Movements in Extended Reality</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>, <a class="a-int" href="https://www.perceptualui.org/people/gzhang/">Guanhua Zhang</a>, Zheming Yin, <a class="a-int" href="https://scholar.google.de/citations?user=HO3nVAoAAAAJ&hl=de">Daniel Haeufle</a>, <a class="a-int" href="https://www.imsb.uni-stuttgart.de/team/Schmitt-00006/">Syn Schmitt</a>, <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics, 2025: 1-12.
</span>
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu25_haheae" class="pub_show" onclick="pub_showhide(&#39;hu25_haheae&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu25_haheae" class="pub_show" onclick="pub_showhide(&#39;hu25_haheae&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu25_haheae" class="pub_show" onclick="pub_showhide(&#39;hu25_haheae&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu25_haheae.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu25_haheae" style="display:none;">
Human hand and head movements are the most pervasive input modalities in extended reality (XR) and are significant for a wide range of applications. However, prior works on hand and head modelling in XR only explored a single modality or focused on specific applications. We present HaHeAE - a novel self-supervised method for learning generalisable joint representations of hand and head movements in XR. At the core of our method is an autoencoder (AE) that uses a graph convolutional network-based semantic encoder and a diffusion-based stochastic encoder to learn the joint semantic and stochastic representations of hand-head movements. It also features a diffusion-based decoder to reconstruct the original signals. Through extensive evaluations on three public XR datasets, we show that our method 1) significantly outperforms commonly used self-supervised methods by up to 74.1% in terms of reconstruction quality and is generalisable across users, activities, and XR environments, 2) enables new applications, including interpretable hand-head cluster identification and variable hand-head movement generation, and 3) can serve as an effective feature extractor for downstream tasks. Together, these results demonstrate the effectiveness of our method and underline the potential of self-supervised methods for jointly modelling hand-head behaviours in extended reality.

</div>
<div class="pub_links bg_grey" id="pub_links_hu25_haheae" style="display:none;">
<div class="pub_links">  
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu25_haheae/pdf/hu25_haheae.pdf">paper.pdf</a></p>
  
</div>
</div>

<div class="pub_bibtex bg_grey" id="pub_bibtex_hu25_haheae" style="display:none;">@article{hu25haheae,
	author={Hu, Zhiming and Zhang, Guanhua and Yin, Zheming and Haeufle, Daniel and Schmitt, Syn and Bulling, Andreas},
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={HaHeAE: Learning Generalisable Joint Representations of Human Hand and Head Movements in Extended Reality}, 
	year={2025},
	pages = {1--12},
	doi={10.1109/TVCG.2025.3576999}}
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu24_hoimotion" class="margin-bottom-30">
<a href="./hu24_hoimotion.html">
<img class="thumbnail" src="./hu24_hoimotion/image/thumb.gif" title="HOIMotion: Forecasting Human Motion During Human-Object Interactions Using Egocentric 3D Object Bounding Boxes" alt="HOIMotion: Forecasting Human Motion During Human-Object
Interactions Using Egocentric 3D Object Bounding Boxes">
</a>	
<p class="pub_title">HOIMotion: Forecasting Human Motion During Human-Object
Interactions Using Egocentric 3D Object Bounding Boxes</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>, Zheming Yin, <a class="a-int" href="https://scholar.google.de/citations?user=HO3nVAoAAAAJ&hl=de">Daniel Haeufle</a>, 
<a class="a-int" href="https://www.imsb.uni-stuttgart.de/team/Schmitt-00006/">Syn Schmitt</a>,
<a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
</p>
<p class="pub_additional">
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics (TVCG, ISMAR 2024 Journal-track), 2024, 30(11): 7375 - 7385.</span>

</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu24_hoimotion" class="pub_show" onclick="pub_showhide(&#39;hu24_hoimotion&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu24_hoimotion" class="pub_show" onclick="pub_showhide(&#39;hu24_hoimotion&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu24_hoimotion" class="pub_show" onclick="pub_showhide(&#39;hu24_hoimotion&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu24_hoimotion.html">Project</a></span>
	<span class="pub_award"><svg class="svg-inline--fa fa-award fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="award" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M97.12 362.63c-8.69-8.69-4.16-6.24-25.12-11.85-9.51-2.55-17.87-7.45-25.43-13.32L1.2 448.7c-4.39 10.77 3.81 22.47 15.43 22.03l52.69-2.01L105.56 507c8 8.44 22.04 5.81 26.43-4.96l52.05-127.62c-10.84 6.04-22.87 9.58-35.31 9.58-19.5 0-37.82-7.59-51.61-21.37zM382.8 448.7l-45.37-111.24c-7.56 5.88-15.92 10.77-25.43 13.32-21.07 5.64-16.45 3.18-25.12 11.85-13.79 13.78-32.12 21.37-51.62 21.37-12.44 0-24.47-3.55-35.31-9.58L252 502.04c4.39 10.77 18.44 13.4 26.43 4.96l36.25-38.28 52.69 2.01c11.62.44 19.82-11.27 15.43-22.03zM263 340c15.28-15.55 17.03-14.21 38.79-20.14 13.89-3.79 24.75-14.84 28.47-28.98 7.48-28.4 5.54-24.97 25.95-45.75 10.17-10.35 14.14-25.44 10.42-39.58-7.47-28.38-7.48-24.42 0-52.83 3.72-14.14-.25-29.23-10.42-39.58-20.41-20.78-18.47-17.36-25.95-45.75-3.72-14.14-14.58-25.19-28.47-28.98-27.88-7.61-24.52-5.62-44.95-26.41-10.17-10.35-25-14.4-38.89-10.61-27.87 7.6-23.98 7.61-51.9 0-13.89-3.79-28.72.25-38.89 10.61-20.41 20.78-17.05 18.8-44.94 26.41-13.89 3.79-24.75 14.84-28.47 28.98-7.47 28.39-5.54 24.97-25.95 45.75-10.17 10.35-14.15 25.44-10.42 39.58 7.47 28.36 7.48 24.4 0 52.82-3.72 14.14.25 29.23 10.42 39.59 20.41 20.78 18.47 17.35 25.95 45.75 3.72 14.14 14.58 25.19 28.47 28.98C104.6 325.96 106.27 325 121 340c13.23 13.47 33.84 15.88 49.74 5.82a39.676 39.676 0 0 1 42.53 0c15.89 10.06 36.5 7.65 49.73-5.82zM97.66 175.96c0-53.03 42.24-96.02 94.34-96.02s94.34 42.99 94.34 96.02-42.24 96.02-94.34 96.02-94.34-42.99-94.34-96.02z"></path></svg> Best Journal Paper Award</span>	
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu24_hoimotion" style="display:none;">
We present HOIMotion – a novel approach for human motion forecasting during human-object interactions that integrates information about past body poses and egocentric 3D object bounding boxes. Human motion forecasting is important in many augmented reality applications but most existing methods have only used past body poses to predict future motion. HOIMotion first uses an encoder-residual graph convolutional network (GCN) and multi-layer perceptrons to extract features from body poses and egocentric 3D object bounding boxes, respectively. Our method then fuses pose and object features into a novel pose-object graph and uses a residual-decoder GCN to forecast future body motion. We extensively evaluate our method on the Aria digital twin (ADT) and MoGaze datasets and show that HOIMotion consistently outperforms state-of-the-art methods by a large margin of up to 8.7% on ADT and 7.2% on MoGaze in terms of mean per joint position error. Complementing these evaluations, we report a human study (N=20) that shows that the improvements achieved by our method result in forecasted poses being perceived as both more precise and more realistic than those of existing methods. Taken together, these results reveal the significant information content available in egocentric 3D object bounding boxes for human motion forecasting and the effectiveness of our method in exploiting this information.
</div>
<div class="pub_links bg_grey" id="pub_links_hu24_hoimotion" style="display:none;">
<div class="pub_links">
  <!--<i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2021.3138902" rel="nofollow" target="_blank">doi</a></p>-->
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu24_hoimotion/pdf/hu24_hoimotion.pdf">paper.pdf</a></p>	
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu24_hoimotion" style="display:none;">@article{hu24hoimotion,
	author={Hu, Zhiming and Yin, Zheming and Haeufle, Daniel and Schmitt, Syn and Bulling, Andreas},
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={HOIMotion: Forecasting Human Motion During Human-Object Interactions Using Egocentric 3D Object Bounding Boxes}, 
	year={2024},
	volume={30},
	number={11},
	pages={7375--7385},
	doi={10.1109/TVCG.2024.3456161}}	
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu24_pose2gaze" class="margin-bottom-30">
<a href="./hu24_pose2gaze.html">
<img class="thumbnail" src="./hu24_pose2gaze/image/thumb.gif" title="Pose2Gaze: Eye-body Coordination during Daily Activities for Gaze Prediction from Full-body Poses" alt="Pose2Gaze: Eye-body Coordination during Daily Activities for Gaze Prediction from Full-body Poses">
</a>	
<p class="pub_title">Pose2Gaze: Eye-body Coordination during Daily Activities for Gaze Prediction from Full-body Poses</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>, Jiahui Xu, 
<a class="a-int" href="https://www.imsb.uni-stuttgart.de/team/Schmitt-00006/">Syn Schmitt</a>,
<a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics (TVCG, oral presentation at ISMAR 2024), 2025, 31(9): 4655-4666.
</span>
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu24_pose2gaze" class="pub_show" onclick="pub_showhide(&#39;hu24_pose2gaze&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu24_pose2gaze" class="pub_show" onclick="pub_showhide(&#39;hu24_pose2gaze&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu24_pose2gaze" class="pub_show" onclick="pub_showhide(&#39;hu24_pose2gaze&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu24_pose2gaze.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu24_pose2gaze" style="display:none;">
Human eye gaze plays a significant role in many virtual and augmented reality (VR/AR) applications, such as gaze-contingent rendering, gaze-based interaction, or eye-based activity recognition. However, prior works on gaze analysis and prediction have only explored eye-head coordination and were limited to human-object interactions. We first report a comprehensive analysis of eye-body coordination in various human-object and human-human interaction activities based on four public datasets collected in real-world (MoGaze), VR (ADT), as well as AR (GIMO and EgoBody) environments. We show that in human-object interactions, e.g. pick and place, eye gaze exhibits strong correlations with full-body motion while in human-human interactions, e.g. chat and teach, a person’s gaze direction is correlated with the body orientation towards the interaction partner. Informed by these analyses we then present Pose2Gaze – a novel eye-body coordination model that uses a convolutional neural network and a spatio-temporal graph convolutional neural network to extract features from head direction and full-body poses, respectively, and then uses a convolutional neural network to predict eye gaze. We compare our method with state-of-the-art methods that predict eye gaze only from head movements and show that Pose2Gaze outperforms these baselines with an average improvement of 24.0% on MoGaze, 10.1% on ADT, 21.3% on GIMO, and 28.6% on EgoBody in mean angular error, respectively. We also show that our method significantly outperforms prior methods in the sample downstream task of eye-based activity recognition. These results underline the significant information content available in eye-body coordination during daily activities and open up a new direction for gaze prediction.
</div>
<div class="pub_links bg_grey" id="pub_links_hu24_pose2gaze" style="display:none;">
<div class="pub_links">
  <!--<i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2021.3138902" rel="nofollow" target="_blank">doi</a></p>-->
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu24_pose2gaze/pdf/hu24_pose2gaze.pdf">paper.pdf</a></p>
  
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu24_pose2gaze" style="display:none;">@article{hu24pose2gaze,
	author={Hu, Zhiming and Xu, Jiahui and Schmitt, Syn and Bulling, Andreas},
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={Pose2Gaze: Eye-body Coordination during Daily Activities for Gaze Prediction from Full-body Poses}, 
	year={2025},
	volume={31},
	number={9},
	pages={4655--4666},
	doi={10.1109/TVCG.2024.3412190}}	
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="wang24_visrecall" class="margin-bottom-30">
        <a href="./wang24_visrecall.html">
        <img class="thumbnail" src="./wang24_visrecall/image/thumb.png" title="VisRecall++: Analysing and Predicting Visualisation Recallability from Gaze Behaviour" alt="VisRecall++: Analysing and Predicting Visualisation Recallability from Gaze Behaviour">
        </a>
    
    <p class="pub_title">VisRecall++: Analysing and Predicting Visualisation Recallability from Gaze Behaviour</p>

    <p class="pub_author">	
            <a class="a-int" href="https://www.perceptualui.org/people/wang/">Yao Wang</a>, 
			<a class="a-int" href="https://yuejiang-nj.github.io/">Yue Jiang</a>, 
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,    
			<a class="a-int" href="https://www.perceptualui.org/people/ruhdorfer/">Constantin Ruhdorfer</a>,
			<a class="a-int" href="https://www.perceptualui.org/people/bace/">Mihai Bâce</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
    </p>
	
    <p class="pub_additional">
        
		<span class="pub_additional_journal">Proceedings of the ACM on Human-Computer Interaction (PACM HCI), 2024, 8(ETRA): 1-18.
		</span>
    </p>

    <p class="pub_tags">
        
        <span class="button bg_greydark"><a id="pub_abstract_sh_wang24_visrecall" class="pub_show" onclick="pub_showhide('wang24_visrecall','pub_abstract')">Abstract</a></span>
                
        <span class="button bg_greydark"><a id="pub_links_sh_wang24_visrecall" class="pub_show" onclick="pub_showhide('wang24_visrecall','pub_links')">Links</a></span>
        
        <span class="button bg_greydark"><a id="pub_bibtex_sh_wang24_visrecall" class="pub_show" onclick="pub_showhide('wang24_visrecall','pub_bibtex')">BibTeX</a></span>

        <span class="button bg_greydark margin-left-10"><a href="./wang24_visrecall.html">Project</a></span>
        
    </p>
	
    <div class="pub_abstract bg_grey" id="pub_abstract_wang24_visrecall" style="display:none;">
        Question answering has recently been proposed as a promising means to assess the recallability of information visualisations. However, prior works are yet to study the link between visually encoding a visualisation in memory and recall performance. To fill this gap, we propose VisRecall++ – a novel 40-participant recallability dataset that contains gaze data on 200 visualisations and five question types, such as identifying the title, and finding extreme values.We measured recallability by asking participants questions after they observed the visualisation for 10 seconds.Our analyses reveal several insights, such as saccade amplitude, number of fixations, and fixation duration significantly differ between high and low recallability groups.Finally, we propose GazeRecallNet – a novel computational method to predict recallability from gaze behaviour that outperforms several baselines on this task.Taken together, our results shed light on assessing recallability from gaze behaviour and inform future work on recallability-based visualisation optimisation.
    </div>

    <div class="pub_links bg_grey" id="pub_links_wang24_visrecall" style="display:none;">
        <div class="pub_links">                            
	<i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./wang24_visrecall/pdf/wang24_visrecall.pdf">paper.pdf</a></p>                               
        </div>
    </div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_wang24_visrecall" style="display:none;">@article{wang24visrecall,
	title = {VisRecall++: Analysing and Predicting Visualisation Recallability from Gaze Behaviour},
	author = {Wang, Yao and Jiang, Yue and Hu, Zhiming and Ruhdorfer, Constantin and Bâce, Mihai and Bulling, Andreas},
	year = {2024},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	pages = {1--18},
	volume = {8},
	number = {ETRA}}
</div>

</div>
</li></ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="elfares24_privateyes" class="margin-bottom-30">
        <a href="./elfares24_privateyes.html">
        <img class="thumbnail" src="./elfares24_privateyes/image/thumb.png" title="PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure Multi-Party Computation" alt="PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure Multi-Party Computation">
        </a>
    
    <p class="pub_title">PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure Multi-Party Computation</p>

    <p class="pub_author">
            <a class="a-int" href="https://www.perceptualui.org/people/elfares/">Mayar Elfares</a>, 
			Pascal Reisert, 
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,    Wenwu Tang, Ralf Küsters, 
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
    </p>
	
    <p class="pub_additional">
        
		<span class="pub_additional_journal">Proceedings of the ACM on Human-Computer Interaction (PACM HCI), 2024, 8(ETRA): 1-23.
		</span>
    </p>
	
    <p class="pub_tags">
        
        <span class="button bg_greydark"><a id="pub_abstract_sh_elfares24_privateyes" class="pub_show" onclick="pub_showhide('elfares24_privateyes','pub_abstract')">Abstract</a></span>
                
        <span class="button bg_greydark"><a id="pub_links_sh_elfares24_privateyes" class="pub_show" onclick="pub_showhide('elfares24_privateyes','pub_links')">Links</a></span>
        
        <span class="button bg_greydark"><a id="pub_bibtex_sh_elfares24_privateyes" class="pub_show" onclick="pub_showhide('elfares24_privateyes','pub_bibtex')">BibTeX</a></span>

        <span class="button bg_greydark margin-left-10"><a href="./elfares24_privateyes.html">Project</a></span>
        
    </p>
	
    <div class="pub_abstract bg_grey" id="pub_abstract_elfares24_privateyes" style="display:none;">
        Latest gaze estimation methods require large-scale training data but their collection and exchange pose significant privacy risks. We propose PrivatEyes - the first privacy-enhancing training approach for appearance-based gaze estimation based on federated learning (FL) and secure multi-party computation (MPC). PrivatEyes enables training gaze estimators on multiple local datasets across different users and server-based secure aggregation of the individual estimators’ updates. PrivatEyes guarantees that individual gaze data remains private even if a majority of the aggregating servers is malicious. We also introduce a new data leakage attack DualView that shows that PrivatEyes limits the leakage of private training data more effectively than previous approaches. Evaluations on the MPIIGaze, MPIIFaceGaze, GazeCapture, and NVGaze datasets further show that the improved privacy does not lead to a lower gaze estimation accuracy or substantially higher computational costs - both of which are on par with its non-secure counterparts.
    </div>

    <div class="pub_links bg_grey" id="pub_links_elfares24_privateyes" style="display:none;">
        <div class="pub_links">                            
	<i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./elfares24_privateyes/pdf/elfares24_privateyes.pdf">paper.pdf</a></p>                               
        </div>
    </div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_elfares24_privateyes" style="display:none;">@article{elfares24privateyes,
	title = {PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure Multi-Party Computation},
	author = {Elfares, Mayar and Reisert, Pascal and Hu, Zhiming and Tang, Wenwu and Küsters, Ralf and Bulling, Andreas},
	year = {2024},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	pages = {1--23},
	volume = {8},
	number = {ETRA}}
</div>

</div>
</li></ol>


<!--paper-->
<ol class="bibliography"><li>
  <div id="lin22_intentional" class="margin-bottom-30">
  
      
      
          <a href="./lin22_intentional.html">
          <img class="thumbnail" src="./lin22_intentional/image/thumb.png" title="Intentional Head-Motion Assisted Locomotion for Reducing Cybersickness" alt="Intentional Head-Motion Assisted Locomotion for Reducing Cybersickness">
          </a>
      
  
      <p class="pub_title">Intentional Head-Motion Assisted Locomotion for Reducing Cybersickness</p>
  
      <p class="pub_author">
          
              Zehui Lin,
              Xiang Gu,
              Sheng Li,
              <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
              Guoping Wang
      </p>
  
      <p class="pub_additional">
          
		  <span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics (TVCG, oral presentation at IEEE VR 2022), 2023, 29(8): 3458-3471.
          </span>
      </p>
  
      <p class="pub_tags">
          
          <span class="button bg_greydark"><a id="pub_abstract_sh_lin22_intentional" class="pub_show" onclick="pub_showhide('lin22_intentional','pub_abstract')">Abstract</a></span>
          
  
          
          <span class="button bg_greydark"><a id="pub_links_sh_lin22_intentional" class="pub_show" onclick="pub_showhide('lin22_intentional','pub_links')">Links</a></span>
          
  
          <span class="button bg_greydark"><a id="pub_bibtex_sh_lin22_intentional" class="pub_show" onclick="pub_showhide('lin22_intentional','pub_bibtex')">BibTeX</a></span>
  
          <span class="button bg_greydark margin-left-10"><a href="./lin22_intentional.html">Project</a></span>
  
          
      </p>
  
      <div class="pub_abstract bg_grey" id="pub_abstract_lin22_intentional" style="display:none;">
          We present an efficient locomotion technique that can reduce cybersickness through aligning the visual and vestibular induced self-motion illusion. Our locomotion technique stimulates proprioception consistent with the visual sense by intentional head motion, which includes both the head’s translational movement and yaw rotation. A locomotion event is triggered by the hand-held controller together with an intended physical head motion simultaneously. Based on our method, we further explore the connections between the level of cybersickness and the velocity of self motion through a series of experiments. We first conduct Experiment 1 to investigate the cybersickness induced by different translation velocities using our method and then conduct Experiment 2 to investigate the cybersickness induced by different angular velocities. Our user studies from these two experiments reveal a new finding on the correlation between translation/angular velocities and the level of cybersickness. The cybersickness is greatest at the lowest velocity using our method, and the statistical analysis also indicates a possible U-shaped relation between the translation/angular velocity and cybersickness degree. Finally, we conduct Experiment 3 to evaluate the performances of our method and other commonly-used locomotion approaches, i.e., joystick-based steering and teleportation. The results show that our method can significantly reduce cybersickness compared with the joystick-based steering and obtain a higher presence compared with the teleportation. These advantages demonstrate that our method can be an optional locomotion solution for immersive VR applications using commercially available HMD suites only.
      </div>
  
      <div class="pub_links bg_grey" id="pub_links_lin22_intentional" style="display:none;">
          <div class="pub_links">
                  
                      <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="https://ieeexplore.ieee.org/document/9737429" rel="nofollow" target="_blank">doi</a></p>
                  
                  
                      <i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./lin22_intentional/pdf/lin22_intentional.pdf">paper.pdf</a></p>
                  
                 
          </div>
      </div>
  <div class="pub_bibtex bg_grey" id="pub_bibtex_lin22_intentional" style="display:none;">@article{lin22intentional,
	author={Lin, Zehui and Gu, Xiang and Li, Sheng and Hu, Zhiming and Wang, Guoping},
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={Intentional Head-Motion Assisted Locomotion for Reducing Cybersickness}, 
	year={2023},
	volume={29},
	number={8},
	pages={3458--3471},
	doi={10.1109/TVCG.2022.3160232}}
</div>
  </li></ol>

  
<!--paper-->
<ol class="bibliography"><li>
<div id="hu22_ehtask" class="margin-bottom-30">
<a href="./hu22_ehtask.html">
<img class="thumbnail" src="./hu22_ehtask/image/thumb.png" title="EHTask: Recognizing User Tasks from Eye and Head Movements in Immersive Virtual Reality" alt="EHTask: Recognizing User Tasks from Eye and Head Movements in Immersive Virtual Reality">
</a>	
<p class="pub_title">EHTask: Recognizing User Tasks from Eye and Head Movements in Immersive Virtual Reality</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
<a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>,
Sheng Li, Guoping Wang
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics (TVCG, oral presentation at IEEE VR 2022), 2023, 29(4): 1992-2004.
</span>
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu22_ehtask" class="pub_show" onclick="pub_showhide(&#39;hu22_ehtask&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu22_ehtask" class="pub_show" onclick="pub_showhide(&#39;hu22_ehtask&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu22_ehtask" class="pub_show" onclick="pub_showhide(&#39;hu22_ehtask&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu22_ehtask.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu22_ehtask" style="display:none;">
Understanding human visual attention in immersive virtual reality (VR) is crucial for many important applications, including gaze prediction, gaze guidance, and gaze-contingent rendering.
However, previous works on visual attention analysis typically only explored one specific VR task and paid less attention to the differences between different tasks.
Moreover, existing task recognition methods typically focused on 2D viewing conditions and only explored the effectiveness of human eye movements.
We first collect eye and head movements of 30 participants performing four tasks, i.e. Free viewing, Visual search, Saliency, and Track, in 15 360-degree VR videos.
Using this dataset, we analyze the patterns of human eye and head movements and reveal significant differences across different tasks in terms of fixation duration, saccade amplitude, head rotation velocity, and eye-head coordination.
We then propose EHTask -- a novel learning-based method that employs eye and head movements to recognize user tasks in VR.
We show that our method significantly outperforms the state-of-the-art methods derived from 2D viewing conditions both on our dataset (accuracy of 84.4% vs. 62.8%) and on a real-world dataset (61.9% vs. 44.1%). 
As such, our work provides meaningful insights into human visual attention under different VR tasks and guides future work on recognizing user tasks in VR.
</div>
<div class="pub_links bg_grey" id="pub_links_hu22_ehtask" style="display:none;">
<div class="pub_links">
  <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2021.3138902" rel="nofollow" target="_blank">doi</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu22_ehtask/pdf/hu22_ehtask.pdf">paper.pdf</a></p>
  <i class="fa fa-database"></i>&nbsp;&nbsp;Dataset: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FEHTaskDataset&ga=1">dataset</a></p>
  <i class="fa fa-file-powerpoint"></i>&nbsp; Slides: <a class="pub_list" href="./hu22_ehtask/ppt/hu22_ehtask.pdf">slides.pdf</a></p>
  <i class="fa fa-code"></i>&nbsp;Code: <a class="a-text-ext" href="https://github.com/CraneHzm/EHTask">code</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu22_ehtask" style="display:none;">@article{hu22ehtask,
	author={Hu, Zhiming and Bulling, Andreas and Li, Sheng and Wang, Guoping},
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={EHTask: Recognizing User Tasks From Eye and Head Movements in Immersive Virtual Reality}, 
	year={2023},
	volume={29},
	number={4},
	pages={1992--2004},
	doi={10.1109/TVCG.2021.3138902}}  
</div>
</ol>


<!--paper-->
<!--<ol class="bibliography"><li>
<div id="hu21_user" class="margin-bottom-30">
<a href="./hu21_user.html">
<img class="thumbnail" src="./hu21_user/image/thumb.png" title="Research progress of user task prediction and algorithm analysis (in Chinese)" alt="Research progress of user task prediction and algorithm analysis (in Chinese)">
</a>	
<p class="pub_title">Research progress of user task prediction and algorithm analysis (in Chinese)</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
Sheng Li, Meng Gai
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">Journal of Graphics, 2021, 42(3): 367-375.
</span>
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu21_user" class="pub_show" onclick="pub_showhide(&#39;hu21_user&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu21_user" class="pub_show" onclick="pub_showhide(&#39;hu21_user&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu21_user" class="pub_show" onclick="pub_showhide(&#39;hu21_user&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu21_user.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu21_user" style="display:none;">
Users’ cognitive behaviors are dramatically influenced by the specific tasks assigned to them. 
Information on users’ tasks can be applied to many areas, such as human behavior analysis and intelligent human-computer interfaces. 
It can be used as the input of intelligent systems and enable the systems to automatically adjust their functions according to different tasks. 
User task prediction refers to the prediction of users’ tasks at hand based on the characteristics of his or her eye movements, the characteristics of scene content, and other related information. 
User task prediction is a popular research topic in vision research, and researchers have proposed many successful task prediction algorithms. 
However, the algorithms proposed in prior works mainly focus on a particular scene, and comparison and analysis are absent for these algorithms. 
This paper presented a review of prior works on task prediction in scenes of images, videos, and real world, and detailed existing task prediction algorithms. 
Based on a real-world task dataset, this paper evaluated the performances of existing algorithms and conducted the corresponding analysis and discussion. 
As such, this work can provide meaningful insights for future works on this important topic.
</div>
<div class="pub_links bg_grey" id="pub_links_hu21_user" style="display:none;">
<div class="pub_links">
  <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://www.txxb.com.cn/CN/10.11996/JG.j.2095-302X.2021030367" rel="nofollow" target="_blank">doi</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu21_user/pdf/hu21_user.pdf">paper.pdf</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu21_user" style="display:none;">@article{hu21user,
	title = {Research progress of user task prediction and algorithm analysis (in Chinese)},
	author = {Hu, Zhiming and Li, Sheng and Gai, Meng},
	year = {2021},
	journal={Journal of Graphics},
	doi = {http://www.txxb.com.cn/CN/10.11996/JG.j.2095-302X.2021030367},
	volume = {42},
	number = {3},
	pages = {367-375}}
</div>
</ol>-->


<!--paper-->
<ol class="bibliography"><li>
<div id="hu21_fixationnet" class="margin-bottom-30">
<a href="./hu21_fixationnet.html">
<img class="thumbnail" src="./hu21_fixationnet/image/thumb.gif" title="FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments" alt="FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments">
</a>
<p class="pub_title">FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
<a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>,
Sheng Li, Guoping Wang
</p>
<p class="pub_additional">
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics (TVCG, IEEE VR 2021 Journal-track), 2021, 27(5): 2681-2690.
</span>
</p>
<p class="pub_tags">        
<span class="button bg_greydark"><a id="pub_abstract_sh_hu21_fixationnet" class="pub_show" onclick="pub_showhide(&#39;hu21_fixationnet&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
<span class="button bg_greydark"><a id="pub_links_sh_hu21_fixationnet" class="pub_show" onclick="pub_showhide(&#39;hu21_fixationnet&#39;,&#39;pub_links&#39;)">Links</a></span>
<span class="button bg_greydark"><a id="pub_bibtex_sh_hu21_fixationnet" class="pub_show" onclick="pub_showhide(&#39;hu21_fixationnet&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
<span class="button bg_greydark margin-left-10"><a href="./hu21_fixationnet.html">Project</a></span>
<span class="pub_award"><svg class="svg-inline--fa fa-award fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="award" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M97.12 362.63c-8.69-8.69-4.16-6.24-25.12-11.85-9.51-2.55-17.87-7.45-25.43-13.32L1.2 448.7c-4.39 10.77 3.81 22.47 15.43 22.03l52.69-2.01L105.56 507c8 8.44 22.04 5.81 26.43-4.96l52.05-127.62c-10.84 6.04-22.87 9.58-35.31 9.58-19.5 0-37.82-7.59-51.61-21.37zM382.8 448.7l-45.37-111.24c-7.56 5.88-15.92 10.77-25.43 13.32-21.07 5.64-16.45 3.18-25.12 11.85-13.79 13.78-32.12 21.37-51.62 21.37-12.44 0-24.47-3.55-35.31-9.58L252 502.04c4.39 10.77 18.44 13.4 26.43 4.96l36.25-38.28 52.69 2.01c11.62.44 19.82-11.27 15.43-22.03zM263 340c15.28-15.55 17.03-14.21 38.79-20.14 13.89-3.79 24.75-14.84 28.47-28.98 7.48-28.4 5.54-24.97 25.95-45.75 10.17-10.35 14.14-25.44 10.42-39.58-7.47-28.38-7.48-24.42 0-52.83 3.72-14.14-.25-29.23-10.42-39.58-20.41-20.78-18.47-17.36-25.95-45.75-3.72-14.14-14.58-25.19-28.47-28.98-27.88-7.61-24.52-5.62-44.95-26.41-10.17-10.35-25-14.4-38.89-10.61-27.87 7.6-23.98 7.61-51.9 0-13.89-3.79-28.72.25-38.89 10.61-20.41 20.78-17.05 18.8-44.94 26.41-13.89 3.79-24.75 14.84-28.47 28.98-7.47 28.39-5.54 24.97-25.95 45.75-10.17 10.35-14.15 25.44-10.42 39.58 7.47 28.36 7.48 24.4 0 52.82-3.72 14.14.25 29.23 10.42 39.59 20.41 20.78 18.47 17.35 25.95 45.75 3.72 14.14 14.58 25.19 28.47 28.98C104.6 325.96 106.27 325 121 340c13.23 13.47 33.84 15.88 49.74 5.82a39.676 39.676 0 0 1 42.53 0c15.89 10.06 36.5 7.65 49.73-5.82zM97.66 175.96c0-53.03 42.24-96.02 94.34-96.02s94.34 42.99 94.34 96.02-42.24 96.02-94.34 96.02-94.34-42.99-94.34-96.02z"></path></svg> Best Journal Paper Nominees</span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu21_fixationnet" style="display:none;">
Human visual attention in immersive virtual reality (VR) is key for many important applications, such as content design, gaze-contingent rendering, or gaze-based interaction.
However, prior works typically focused on free-viewing conditions that have limited relevance for practical applications.
We first collect eye tracking data of 27 participants performing a visual search task in four immersive VR environments.
Based on this dataset, we provide a comprehensive analysis of the collected data and reveal correlations between users' eye fixations and other factors, i.e. users' historical gaze positions, task-related objects, saliency information of the VR content, and users' head rotation velocities.
Based on this analysis, we propose <i>FixationNet</i> -- a novel learning-based model to forecast users' eye fixations in the near future in VR.
We evaluate the performance of our model for free-viewing and task-oriented settings and show that it outperforms the state of the art by a large margin of 19.8% (from a mean error of 2.93&deg to 2.35&deg) in free-viewing and of 15.1% (from 2.05&deg to 1.74&deg) in task-oriented situations.
As such, our work provides new insights into task-oriented attention in virtual environments and guides future work on this important topic in VR research.</div>
<div class="pub_links bg_grey" id="pub_links_hu21_fixationnet" style="display:none;">
<div class="pub_links">            
  <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2021.3067779" rel="nofollow" target="_blank">doi</a></p>
            
            
  <i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./hu21_fixationnet/pdf/hu21_fixationnet.pdf">paper.pdf</a></p>



  <i class="fa fa-link"></i><p>Code: <a class="a-text-ext" href="https://github.com/CraneHzm/FixationNet" rel="nofollow" target="_blank">code</a></p>            

  <i class="fa fa-link"></i><p>Dataset: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FFixationNetDataset&ga=1" rel="nofollow" target="_blank">dataset</a></p>

  <i class="fa fa-file-pdf"></i>&nbsp;Slides: <a class="pub_list" href="./hu21_fixationnet/ppt/hu21_fixationnet.pdf">slides.pdf</a></p>

  <i class="fa fa-link"></i><p>Experimental scenes: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FFixationNetScenes&ga=1" rel="nofollow" target="_blank">experimental senes</a></p>

  <i class="fa fa-link"></i><p>Supplementary materials: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FFixationNet%5FSupplementalMaterial%2Ezip&parent=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments&ga=1" rel="nofollow" target="_blank">supplementary materials</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu21_fixationnet" style="display:none;">@article{hu21fixationnet,
	title={FixationNet: Forecasting eye fixations in task-oriented virtual environments},
	author={Hu, Zhiming and Bulling, Andreas and Li, Sheng and Wang, Guoping},
	journal={IEEE Transactions on Visualization and Computer Graphics},
	volume={27},
	number={5},
	pages={2681--2690},
	year={2021},
	publisher={IEEE}}
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu20_dgaze" class="margin-bottom-30">
<a href="./hu20_dgaze.html">
<img class="thumbnail" src="./hu20_dgaze/image/thumb.gif" title="DGaze: CNN-Based Gaze Prediction in Dynamic Scenes" alt="DGaze: CNN-Based Gaze Prediction in Dynamic Scenes">
</a>	
<p class="pub_title">DGaze: CNN-Based Gaze Prediction in Dynamic Scenes</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>, 
Sheng Li,
<a class="a-int" href="https://cong-yi.github.io/">Congyi Zhang</a>,
Kangrui Yi, Guoping Wang,
<a class="a-int" href="https://www.cs.umd.edu/people/dmanocha">Dinesh Manocha</a>
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics (TVCG, IEEE VR 2020 Journal-track), 2020, 26(5): 1902-1911.
</span>
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu20_dgaze" class="pub_show" onclick="pub_showhide(&#39;hu20_dgaze&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu20_dgaze" class="pub_show" onclick="pub_showhide(&#39;hu20_dgaze&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu20_dgaze" class="pub_show" onclick="pub_showhide(&#39;hu20_dgaze&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu20_dgaze.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu20_dgaze" style="display:none;">
We conduct novel analyses of users' gaze behaviors in dynamic virtual scenes and, based on our analyses, we present a novel CNN-based model called DGaze for gaze prediction in HMD-based applications. 
We first collect 43 users' eye tracking data in 5 dynamic scenes under free-viewing conditions. 
Next, we perform statistical analysis of our data and observe that dynamic object positions, head rotation velocities, and salient regions are correlated with users' gaze positions. 
Based on our analysis, we present a CNN-based model (DGaze) that combines object position sequence, head velocity sequence, and saliency features to predict users' gaze positions. 
Our model can be applied to predict not only realtime gaze positions but also gaze positions in the near future and can achieve better performance than prior method. 
In terms of realtime prediction, DGaze achieves a 22.0% improvement over prior method in dynamic scenes and obtains an improvement of 9.5% in static scenes, based on using the angular distance as the evaluation metric. 
We also propose a variant of our model called DGaze_ET that can be used to predict future gaze positions with higher precision by combining accurate past gaze data gathered using an eye tracker. 
We further analyze our CNN architecture and verify the effectiveness of each component in our model. 
We apply DGaze to gaze-contingent rendering and a game, and also present the evaluation results from a user study.
</div>
<div class="pub_links bg_grey" id="pub_links_hu20_dgaze" style="display:none;">
<div class="pub_links">
  <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2020.2973473" rel="nofollow" target="_blank">doi</a></p>
            
            
  <i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./hu20_dgaze/pdf/hu20_dgaze.pdf">paper.pdf</a></p>

  <i class="fa fa-link"></i><p>Code: <a class="a-text-ext" href="https://github.com/CraneHzm/DGaze" rel="nofollow" target="_blank">code</a></p>            

  <i class="fa fa-link"></i><p>Dataset: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FDGazeDataset&ga=1" rel="nofollow" target="_blank">dataset</a></p>

  <i class="fa fa-file-pdf"></i>&nbsp;Slides: <a class="pub_list" href="./hu20_dgaze/ppt/hu20_dgaze.pdf">slides.pdf</a></p>

  <i class="fa fa-link"></i><p>Experimental scenes: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FDGazeScenes&ga=1" rel="nofollow" target="_blank">experimental scenes</a></p>

  <i class="fa fa-link"></i><p>Supplementary materials: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FDGaze%5FSupplementalMaterial%2Ezip&parent=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments&ga=1" rel="nofollow" target="_blank">supplementary materials</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu20_dgaze" style="display:none;">@article{hu20dgaze,
	title={DGaze: CNN-Based Gaze Prediction in Dynamic Scenes},
	author={Hu, Zhiming and Li, Sheng and Zhang, Congyi and Yi, Kangrui and Wang, Guoping and Manocha, Dinesh},
	journal={IEEE Transactions on Visualization and Computer Graphics},
	volume={26},
	number={5},
	pages={1902--1911},
	year={2020},
	publisher={IEEE}}
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu20_temporal" class="margin-bottom-30">
<a href="./hu20_temporal.html">
<img class="thumbnail" src="./hu20_temporal/image/thumb.png" title="Temporal continuity of visual attention for future gaze prediction in immersive virtual reality" alt="Temporal continuity of visual attention for future gaze prediction in immersive virtual reality">
</a>	
<p class="pub_title">Temporal continuity of visual attention for future gaze prediction in immersive virtual reality</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
Sheng Li, Meng Gai
</p>
<p class="pub_additional">
<span class="pub_additional_journal">Virtual Reality and Intelligent Hardware (VRIH), 2020, 2(2): 142-152.
</span>
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu20_temporal" class="pub_show" onclick="pub_showhide(&#39;hu20_temporal&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu20_temporal" class="pub_show" onclick="pub_showhide(&#39;hu20_temporal&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu20_temporal" class="pub_show" onclick="pub_showhide(&#39;hu20_temporal&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu20_temporal.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu20_temporal" style="display:none;">
<b>Background</b> Eye tracking technology is receiving increased attention in the field of virtual reality. 
Specifically, future gaze prediction is crucial in pre-computation for many applications such as gaze-contingent rendering, advertisement placement, and content-based design. 
To explore future gaze prediction, it is necessary to analyze the temporal continuity of visual attention in immersive virtual reality. 
<b>Methods</b> In this paper, the concept of temporal continuity of visual attention is presented. 
Subsequently, an autocorrelation function method is proposed to evaluate the temporal continuity. 
Thereafter, the temporal continuity is analyzed in both free-viewing and task-oriented conditions. 
<b>Results</b> Specifically, in free-viewing conditions, the analysis of a free-viewing gaze dataset indicates that the temporal continuity performs well only within a short time interval. 
A task-oriented game scene condition was created and conducted to collect users' gaze data. 
An analysis of the collected gaze data finds the temporal continuity has a similar performance with that of the free-viewing conditions. 
Temporal continuity can be applied to future gaze prediction and if it is good, users' current gaze positions can be directly utilized to predict their gaze positions in the future. 
<b>Conclusions</b> The current gaze's future prediction performances are further evaluated in both free-viewing and task-oriented conditions and discover that the current gaze can be efficiently applied to the task of short-term future gaze prediction. 
The task of long-term gaze prediction still remains to be explored.
</div>
<div class="pub_links bg_grey" id="pub_links_hu20_temporal" style="display:none;">
<div class="pub_links">
	<svg class="svg-inline--fa fa-fingerprint fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="fingerprint" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M256.12 245.96c-13.25 0-24 10.74-24 24 1.14 72.25-8.14 141.9-27.7 211.55-2.73 9.72 2.15 30.49 23.12 30.49 10.48 0 20.11-6.92 23.09-17.52 13.53-47.91 31.04-125.41 29.48-224.52.01-13.25-10.73-24-23.99-24zm-.86-81.73C194 164.16 151.25 211.3 152.1 265.32c.75 47.94-3.75 95.91-13.37 142.55-2.69 12.98 5.67 25.69 18.64 28.36 13.05 2.67 25.67-5.66 28.36-18.64 10.34-50.09 15.17-101.58 14.37-153.02-.41-25.95 19.92-52.49 54.45-52.34 31.31.47 57.15 25.34 57.62 55.47.77 48.05-2.81 96.33-10.61 143.55-2.17 13.06 6.69 25.42 19.76 27.58 19.97 3.33 26.81-15.1 27.58-19.77 8.28-50.03 12.06-101.21 11.27-152.11-.88-55.8-47.94-101.88-104.91-102.72zm-110.69-19.78c-10.3-8.34-25.37-6.8-33.76 3.48-25.62 31.5-39.39 71.28-38.75 112 .59 37.58-2.47 75.27-9.11 112.05-2.34 13.05 6.31 25.53 19.36 27.89 20.11 3.5 27.07-14.81 27.89-19.36 7.19-39.84 10.5-80.66 9.86-121.33-.47-29.88 9.2-57.88 28-80.97 8.35-10.28 6.79-25.39-3.49-33.76zm109.47-62.33c-15.41-.41-30.87 1.44-45.78 4.97-12.89 3.06-20.87 15.98-17.83 28.89 3.06 12.89 16 20.83 28.89 17.83 11.05-2.61 22.47-3.77 34-3.69 75.43 1.13 137.73 61.5 138.88 134.58.59 37.88-1.28 76.11-5.58 113.63-1.5 13.17 7.95 25.08 21.11 26.58 16.72 1.95 25.51-11.88 26.58-21.11a929.06 929.06 0 0 0 5.89-119.85c-1.56-98.75-85.07-180.33-186.16-181.83zm252.07 121.45c-2.86-12.92-15.51-21.2-28.61-18.27-12.94 2.86-21.12 15.66-18.26 28.61 4.71 21.41 4.91 37.41 4.7 61.6-.11 13.27 10.55 24.09 23.8 24.2h.2c13.17 0 23.89-10.61 24-23.8.18-22.18.4-44.11-5.83-72.34zm-40.12-90.72C417.29 43.46 337.6 1.29 252.81.02 183.02-.82 118.47 24.91 70.46 72.94 24.09 119.37-.9 181.04.14 246.65l-.12 21.47c-.39 13.25 10.03 24.31 23.28 24.69.23.02.48.02.72.02 12.92 0 23.59-10.3 23.97-23.3l.16-23.64c-.83-52.5 19.16-101.86 56.28-139 38.76-38.8 91.34-59.67 147.68-58.86 69.45 1.03 134.73 35.56 174.62 92.39 7.61 10.86 22.56 13.45 33.42 5.86 10.84-7.62 13.46-22.59 5.84-33.43z"></path></svg><!-- <i class="fa fa-fingerprint"></i> --><p>Doi: <a class="a-text-ext" href="https://doi.org/10.1016/j.vrih.2020.01.002" rel="nofollow" target="_blank">doi</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><!-- <i class="fa fa-file-pdf"></i> --><p>Paper: <a class="pub_list" href="./hu20_temporal/pdf/hu20_temporal.pdf">paper.pdf</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu20_temporal" style="display:none;">@article{hu20temporal,
	title={Temporal continuity of visual attention for future gaze prediction in immersive virtual reality},
	author={Hu, Zhiming and Li, Sheng and Gai, Meng},
	journal={Virtual Reality and Intelligent Hardware},
	volume={2},
	number={2},
	pages={142--152},
	year={2020},
	publisher={Elsevier}}
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu19_sgaze" class="margin-bottom-30">
<a href="./hu19_sgaze.html">
<img class="thumbnail" src="./hu19_sgaze/image/thumb.gif" title="SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction" alt="SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction">
</a>	
<p class="pub_title">SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
<a class="a-int" href="https://cong-yi.github.io/">Congyi Zhang</a>,
Sheng Li, Guoping Wang,
<a class="a-int" href="https://www.cs.umd.edu/people/dmanocha">Dinesh Manocha</a>
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics (TVCG, IEEE VR 2019 Journal-track), 2019, 25(5): 2002-2010.
</span>
</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu19_sgaze" class="pub_show" onclick="pub_showhide(&#39;hu19_sgaze&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu19_sgaze" class="pub_show" onclick="pub_showhide(&#39;hu19_sgaze&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu19_sgaze" class="pub_show" onclick="pub_showhide(&#39;hu19_sgaze&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu19_sgaze.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu19_sgaze" style="display:none;">
We present a novel, data-driven eye-head coordination model that can be used for realtime gaze prediction for immersive HMD-based applications without any external hardware or eye tracker. 
Our model (SGaze) is computed by generating a large dataset that corresponds to different users navigating in virtual worlds with different lighting conditions. 
We perform statistical analysis on the recorded data and observe a linear correlation between gaze positions and head rotation angular velocities. 
We also find that there exists a latency between eye movements and head movements. 
SGaze can work as a software-based realtime gaze predictor and we formulate a time related function between head movement and eye movement and use that for realtime gaze position prediction. 
We demonstrate the benefits of SGaze for gaze-contingent rendering and evaluate the results with a user study.
</div>
<div class="pub_links bg_grey" id="pub_links_hu19_sgaze" style="display:none;">
<div class="pub_links">
  <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2019.2899187" rel="nofollow" target="_blank">doi</a></p>
            
            
  <i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./hu19_sgaze/pdf/hu19_sgaze.pdf">paper.pdf</a></p>

  <i class="fa fa-database"></i>&nbsp;&nbsp;Dataset: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FSGaze%5FDataset&ga=1">dataset</a></p>

  <i class="fa fa-code"></i>&nbsp;Code: <a class="a-text-ext" href="https://github.com/CraneHzm/SGaze">code</a></p>

  <i class="fa fa-link"></i><p>Supplementary materials: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FSGaze%5FSupplementalMaterial%2Ezip&parent=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments&ga=1" rel="nofollow" target="_blank">supplementary materials</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu19_sgaze" style="display:none;">@article{hu19sgaze,
	title={SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction},
	author={Hu, Zhiming and Zhang, Congyi and Li, Sheng and Wang, Guoping and Manocha, Dinesh},
	journal={IEEE Transactions on Visualization and Computer Graphics},
	volume={25},
	number={5},
	pages={2002--2010},
	year={2019},
	publisher={IEEE}}
</div>
</ol>


<br><h4>Conference Papers</h4>
<!--paper-->
<ol class="bibliography"><li>
<div id="jiao25_hagi" class="margin-bottom-30">
        <a href="./jiao25_hagi.html">
        <img class="thumbnail" src="./jiao25_hagi/image/thumb.png" title="HAGI: Head-Assisted Gaze Imputation for Mobile Eye Trackers" alt="HAGI: Head-Assisted Gaze Imputation for Mobile Eye Trackers">
        </a>
    
    <p class="pub_title">HAGI: Head-Assisted Gaze Imputation for Mobile Eye Trackers</p>

    <p class="pub_author">
        
            <a class="a-int" href="https://www.perceptualui.org/people/jiao/">Chuhan Jiao</a>,
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
    </p>
	
    <p class="pub_additional">
	
		<span class="pub_additional_journal">Proceedings of the ACM Symposium on User Interface Software and Technology (UIST), 2025: 1-14.		
		</span>        
    </p>

    <p class="pub_tags">
        
        <span class="button bg_greydark"><a id="pub_abstract_sh_jiao25_hagi" class="pub_show" onclick="pub_showhide('jiao25_hagi','pub_abstract')">Abstract</a></span>
                
        <span class="button bg_greydark"><a id="pub_links_sh_jiao25_hagi" class="pub_show" onclick="pub_showhide('jiao25_hagi','pub_links')">Links</a></span>
        
        <span class="button bg_greydark"><a id="pub_bibtex_sh_jiao25_hagi" class="pub_show" onclick="pub_showhide('jiao25_hagi','pub_bibtex')">BibTeX</a></span>

        <span class="button bg_greydark margin-left-10"><a href="./jiao25_hagi.html">Project</a></span>
        
    </p>

    <div class="pub_abstract bg_grey" id="pub_abstract_jiao25_hagi" style="display:none;">
        Mobile eye tracking plays a vital role in capturing human visual attention across both real-world and extended reality (XR) environments, making it an essential tool for applications ranging from behavioural research to human-computer interaction. However, missing values due to blinks, pupil detection errors, or illumination changes pose significant challenges for further gaze data analysis. To address this challenge, we introduce HAGI – a multi-modal diffusion-based approach for gaze data imputation that, for the first time, uses the integrated head orientation sensors to exploit the inherent correlation between head and eye movements. Our method includes a head-movement feature extraction module alongside a novel hybrid feature fusion mechanism that effectively integrates gaze and head motion features at multiple levels. Additionally, we introduce a tailored loss function to enhance gaze imputation accuracy further. Extensive evaluations on the large-scale Nymeria, Ego-Exo4D, and HOT3D datasets demonstrate that HAGI consistently outperforms conventional interpolation methods and deep learning-based time-series imputation baselines, reducing mean angular error by up to 22%. Furthermore, statistical analyses confirm that HAGI produces gaze velocity distributions that more closely match actual human gaze behaviour than baselines, ensuring more realistic gaze imputations. Our method paves the way for more complete and accurate eye gaze recordings in real-world settings and has significant potential for enhancing gaze-based analysis and interaction across various application domains.
    </div>
	
    <div class="pub_links bg_grey" id="pub_links_jiao25_hagi" style="display:none;">
        <div class="pub_links">                            
	<i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./jiao25_hagi/pdf/jiao25_hagi.pdf">paper.pdf</a></p>                               
        </div>
    </div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_jiao25_hagi" style="display:none;">@inproceedings{jiao25hagi,
	author = {Jiao, Chuhan and Hu, Zhiming and Bulling, Andreas},
	title = {HAGI: Head-Assisted Gaze Imputation for Mobile Eye Trackers},
	booktitle = {Proceedings of the ACM Symposium on User Interface Software and Technology},
	year = {2025},
	pages = {1--14},
	doi = {10.1145/3746059.3747749}}
</div>

</div>
</li></ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu25_hoigaze" class="margin-bottom-30">
<a href="./hu25_hoigaze.html">
<img class="thumbnail" src="./hu25_hoigaze/image/thumb.gif" title="HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended Reality Exploiting Eye-Hand-Head Coordination" alt="HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended Reality Exploiting Eye-Hand-Head Coordination">
</a>	
<p class="pub_title">HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended Reality Exploiting Eye-Hand-Head Coordination</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>, <a class="a-int" href="https://scholar.google.de/citations?user=HO3nVAoAAAAJ&hl=de">Daniel Haeufle</a>, <a class="a-int" href="https://www.imsb.uni-stuttgart.de/team/Schmitt-00006/">Syn Schmitt</a>, <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
</p>
<p class="pub_additional">
<span class="pub_additional_journal">Proceedings of the ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH), 2025: 1-10.</span>

</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu25_hoigaze" class="pub_show" onclick="pub_showhide(&#39;hu25_hoigaze&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu25_hoigaze" class="pub_show" onclick="pub_showhide(&#39;hu25_hoigaze&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu25_hoigaze" class="pub_show" onclick="pub_showhide(&#39;hu25_hoigaze&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu25_hoigaze.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu25_hoigaze" style="display:none;">
We present HOIGaze – a novel learning-based approach for gaze estimation during hand-object interactions (HOI) in extended reality (XR). HOIGaze addresses the challenging HOI setting by building on one key insight: The eye, hand, and head movements are closely coordinated during HOIs and this coordination can be exploited to identify samples that are most useful for gaze estimator training – as such, effectively denoising the training data. This denoising approach is in stark contrast to previous gaze estimation methods that treated all training samples as equal. Specifically, we propose: 1) a novel hierarchical framework that first recognises the hand currently visually attended to and then estimates gaze direction based on the attended hand; 2) a new gaze estimator that uses cross-modal Transformers to fuse head and hand-object features extracted using a convolutional neural network and a spatio-temporal graph convolutional network; and 3) a novel eye-head coordination loss that upgrades training samples belonging to the coordinated eye-head movements. We evaluate HOIGaze on the HOT3D and Aria digital twin (ADT) datasets and show that it significantly outperforms state-of-the-art methods, achieving an average improvement of 15.6% on HOT3D and 6.0% on ADT in mean angular error. To demonstrate the potential of our method, we further report significant performance improvements for the sample downstream task of eye-based activity recognition on ADT. Taken together, our results underline the significant information content available in eye-hand-head coordination and, as such, open up an exciting new direction for learning-based gaze estimation.
</div>
<div class="pub_links bg_grey" id="pub_links_hu25_hoigaze" style="display:none;">
<div class="pub_links">
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu25_hoigaze/pdf/hu25_hoigaze.pdf">paper.pdf</a></p>
	
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu25_hoigaze" style="display:none;">@inproceedings{hu25hoigaze,
	title={HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended Reality Exploiting Eye-Hand-Head Coordination},
	author={Hu, Zhiming and Haeufle, Daniel and Schmitt, Syn and Bulling, Andreas},
	booktitle={Proceedings of the ACM Special Interest Group on Computer Graphics and Interactive Techniques},
	year={2025},
	pages = {1--10}}
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="zhang25_summact" class="margin-bottom-30">
        <a href="./zhang25_summact.html">
        <img class="thumbnail" src="./zhang25_summact/image/thumb.png" title="SummAct: Uncovering User Intentions Through Interactive Behaviour Summarisation" alt="SummAct: Uncovering User Intentions Through Interactive Behaviour Summarisation">
        </a>
    
    <p class="pub_title">SummAct: Uncovering User Intentions Through Interactive Behaviour Summarisation</p>

    <p class="pub_author">
            <a class="a-int" href="https://www.perceptualui.org/people/gzhang/">Guanhua Zhang</a>,
			Mohamed Ahmed,
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>,            
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
    </p>
	
    <p class="pub_additional">        
            <span class="pub_additional_journal">Proceedings of the ACM CHI Conference on Human Factors in Computing Systems (CHI), 2025: 1-17.			
			</span>        
    </p>

    <p class="pub_tags">
        
        <span class="button bg_greydark"><a id="pub_abstract_sh_zhang25_summact" class="pub_show" onclick="pub_showhide('zhang25_summact','pub_abstract')">Abstract</a></span>
                
        <span class="button bg_greydark"><a id="pub_links_sh_zhang25_summact" class="pub_show" onclick="pub_showhide('zhang25_summact','pub_links')">Links</a></span>
        
        <span class="button bg_greydark"><a id="pub_bibtex_sh_zhang25_summact" class="pub_show" onclick="pub_showhide('zhang25_summact','pub_bibtex')">BibTeX</a></span>

        <span class="button bg_greydark margin-left-10"><a href="./zhang25_summact.html">Project</a></span>
        
    </p>
	
    <div class="pub_abstract bg_grey" id="pub_abstract_zhang25_summact" style="display:none;">       
		Recent work has highlighted the potential of modelling interactive behaviour analogously to natural language. We propose interactive behaviour summarisation as a novel computational task and demonstrate its usefulness for automatically uncovering latent user goals while interacting with graphical user interfaces. We introduce SummAct – a novel hierarchical method to summarise low-level input actions into high-level goals to tackle this task. SummAct first identifies sub-goals from user actions using a large language model and in-context learning. In a second step, high-level goals are obtained by fine-tuning the model using a novel UI element weighting mechanism to preserve detailed context information embedded within UI elements during summarisation. Through a series of evaluations, we demonstrate that SummAct significantly outperforms baseline methods across desktop and mobile user interfaces and interactive tasks by up to 21.9%. We further introduce two exciting example use cases enabled by our method: interactive behaviour forecasting and automatic behaviour synonym identification.
    </div>

    <div class="pub_links bg_grey" id="pub_links_zhang25_summact" style="display:none;">
        <div class="pub_links">                            
	<i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./zhang25_summact/pdf/zhang25_summact.pdf">paper.pdf</a></p>                               
        </div>
    </div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_zhang25_summact" style="display:none;">@inproceedings{zhang25summact,
	title = {SummAct: Uncovering User Intentions Through Interactive Behaviour Summarisation},
	author = {Zhang, Guanhua and Ahmed, Mohamed and Hu, Zhiming and Bulling, Andreas},
	year = {2025},
	pages = {1--17},
	booktitle = {Proceedings of the ACM CHI Conference on Human Factors in Computing Systems}}
</div>
</div>
</li></ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="yan24_gazemodiff" class="margin-bottom-30">
<a href="./yan24_gazemodiff.html">
<img class="thumbnail" src="./yan24_gazemodiff/image/thumb.png" title="GazeMoDiff: Gaze-guided Diffusion Model for Stochastic Human Motion Prediction" alt="GazeMoDiff: Gaze-guided Diffusion Model for Stochastic Human Motion Prediction">
</a>
<p class="pub_title">GazeMoDiff: Gaze-guided Diffusion Model for Stochastic Human Motion Prediction</p>
<p class="pub_author">
Haodong Yan#, 
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu#*</a>, <a class="a-int" href="https://www.imsb.uni-stuttgart.de/team/Schmitt-00006/">Syn Schmitt</a>, 
<a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
</p>
<p class="pub_additional">
<span class="pub_additional_journal">Proceedings of the Pacific Conference on Computer Graphics and Applications (Pacific Graphics), 2024: 1-12.</span>

</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_yan24_gazemodiff" class="pub_show" onclick="pub_showhide(&#39;yan24_gazemodiff&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_yan24_gazemodiff" class="pub_show" onclick="pub_showhide(&#39;yan24_gazemodiff&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_yan24_gazemodiff" class="pub_show" onclick="pub_showhide(&#39;yan24_gazemodiff&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./yan24_gazemodiff.html">Project</a></span>	
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_yan24_gazemodiff" style="display:none;">
Human motion prediction is important for many virtual and augmented reality (VR/AR) applications such as collision avoidance and realistic avatar generation. Existing methods have synthesised body motion only from observed past motion, despite the fact that human eye gaze is known to correlate strongly with body movements and is readily available in recent VR/AR headsets. We present GazeMoDiff – a novel gaze-guided denoising diffusion model to generate stochastic human motions. Our method first uses a gaze encoder and a motion encoder to extract the gaze and motion features respectively, then employs a graph attention network to fuse these features, and finally injects the gaze-motion features into a noise prediction network via a cross-attention mechanism to progressively generate multiple reasonable human motions in the future. Extensive experiments on the MoGaze and GIMO datasets demonstrate that our method outperforms the state-of-the-art methods by a large margin in terms of multi-modal final displacement error (17.3% on MoGaze and 13.3% on GIMO). We further conducted a human study (N=21) and validated that the motions generated by our method were perceived as both more precise and more realistic than those of prior methods. Taken together, these results reveal the significant information content available in eye gaze for stochastic human motion prediction as well as the effectiveness of our method in exploiting this information.
</div>
<div class="pub_links bg_grey" id="pub_links_yan24_gazemodiff" style="display:none;">
<div class="pub_links">
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./yan24_gazemodiff/pdf/yan24_gazemodiff.pdf">paper.pdf</a></p>
	
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_yan24_gazemodiff" style="display:none;">@inproceedings{yan24gazemodiff,
	title={GazeMoDiff: Gaze-guided Diffusion Model for Stochastic Human Motion Prediction},
	author={Yan, Haodong and Hu, Zhiming and Schmitt, Syn and Bulling, Andreas},
	booktitle={Proceedings of the Pacific Conference on Computer Graphics and Applications},	
	year={2024},
	pages={1--12},
	doi={10.2312/pg.20241315}}
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="zhang24_dismouse" class="margin-bottom-30">
        <a href="./zhang24_dismouse.html">
        <img class="thumbnail" src="./zhang24_dismouse/image/thumb.png" title="DisMouse: Disentangling Information from Mouse Movement Data" alt="DisMouse: Disentangling Information from Mouse Movement Data">
        </a>
    
    <p class="pub_title">DisMouse: Disentangling Information from Mouse Movement Data</p>

    <p class="pub_author">
            <a class="a-int" href="https://www.perceptualui.org/people/gzhang/">Guanhua Zhang</a>,
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
    </p>
	
    <p class="pub_additional">
        
		<span class="pub_additional_journal">Proceedings of the ACM Symposium on User Interface Software and Technology (UIST), 2024: 1-13.		
		</span>        
    </p>

    <p class="pub_tags">
        
        <span class="button bg_greydark"><a id="pub_abstract_sh_zhang24_dismouse" class="pub_show" onclick="pub_showhide('zhang24_dismouse','pub_abstract')">Abstract</a></span>
                
        <span class="button bg_greydark"><a id="pub_links_sh_zhang24_dismouse" class="pub_show" onclick="pub_showhide('zhang24_dismouse','pub_links')">Links</a></span>
        
        <span class="button bg_greydark"><a id="pub_bibtex_sh_zhang24_dismouse" class="pub_show" onclick="pub_showhide('zhang24_dismouse','pub_bibtex')">BibTeX</a></span>

        <span class="button bg_greydark margin-left-10"><a href="./zhang24_dismouse.html">Project</a></span>
        
    </p>
	
    <div class="pub_abstract bg_grey" id="pub_abstract_zhang24_dismouse" style="display:none;">
	Mouse movement data contain rich information about users, performed tasks, and user interfaces, but separating the respective components remains challenging and unexplored. As a first step to address this challenge, we propose DisMouse – the first method to disentangle user-specific and user-independent information and stochastic variations from mouse movement data. At the core of our method is an autoencoder trained in a semi-supervised fashion, consisting of a self-supervised denoising diffusion process and a supervised contrastive user identification module. Through evaluations on three datasets, we show that DisMouse 1) captures complementary information of mouse input, hence providing an interpretable framework for modelling mouse movements, 2) can be used to produce refined features, thus enabling various applications such as personalised and variable mouse data generation, and 3) generalises across different datasets. Taken together, our results underline the significant potential of disentangled representation learning for explainable, controllable, and generalised mouse behaviour modelling.
    </div>
	
    <div class="pub_links bg_grey" id="pub_links_zhang24_dismouse" style="display:none;">
        <div class="pub_links">
	<i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./zhang24_dismouse/pdf/zhang24_dismouse.pdf">paper.pdf</a></p>                               
        </div>
    </div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_zhang24_dismouse" style="display:none;">@inproceedings{zhang24dismouse,
	title = {DisMouse: Disentangling Information from Mouse Movement Data},
	author = {Zhang, Guanhua and Hu, Zhiming and Bulling, Andreas},
	year = {2024},
	pages = {1--13},
	booktitle = {Proceedings of the ACM Symposium on User Interface Software and Technology}}
</div>
</div>
</li></ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu24_gazemotion" class="margin-bottom-30">
<a href="./hu24_gazemotion.html">
<img class="thumbnail" src="./hu24_gazemotion/image/thumb.gif" title="GazeMotion: Gaze-guided Human Motion Forecasting" alt="GazeMotion: Gaze-guided Human Motion Forecasting">
</a>	
<p class="pub_title">GazeMotion: Gaze-guided Human Motion Forecasting</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>, <a class="a-int" href="https://www.imsb.uni-stuttgart.de/team/Schmitt-00006/">Syn Schmitt</a>, <a class="a-int" href="https://scholar.google.de/citations?user=HO3nVAoAAAAJ&hl=de">Daniel Haeufle</a>, 
<a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
</p>
<p class="pub_additional">
<span class="pub_additional_journal">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2024: 13017-13022.</span>

</p>
<p class="pub_tags">
	<span class="button bg_greydark"><a id="pub_abstract_sh_hu24_gazemotion" class="pub_show" onclick="pub_showhide(&#39;hu24_gazemotion&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
	<span class="button bg_greydark"><a id="pub_links_sh_hu24_gazemotion" class="pub_show" onclick="pub_showhide(&#39;hu24_gazemotion&#39;,&#39;pub_links&#39;)">Links</a></span>
	<span class="button bg_greydark"><a id="pub_bibtex_sh_hu24_gazemotion" class="pub_show" onclick="pub_showhide(&#39;hu24_gazemotion&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
	<span class="button bg_greydark margin-left-10"><a href="./hu24_gazemotion.html">Project</a></span>
	<span class="pub_award"><svg class="svg-inline--fa fa-award fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="award" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M97.12 362.63c-8.69-8.69-4.16-6.24-25.12-11.85-9.51-2.55-17.87-7.45-25.43-13.32L1.2 448.7c-4.39 10.77 3.81 22.47 15.43 22.03l52.69-2.01L105.56 507c8 8.44 22.04 5.81 26.43-4.96l52.05-127.62c-10.84 6.04-22.87 9.58-35.31 9.58-19.5 0-37.82-7.59-51.61-21.37zM382.8 448.7l-45.37-111.24c-7.56 5.88-15.92 10.77-25.43 13.32-21.07 5.64-16.45 3.18-25.12 11.85-13.79 13.78-32.12 21.37-51.62 21.37-12.44 0-24.47-3.55-35.31-9.58L252 502.04c4.39 10.77 18.44 13.4 26.43 4.96l36.25-38.28 52.69 2.01c11.62.44 19.82-11.27 15.43-22.03zM263 340c15.28-15.55 17.03-14.21 38.79-20.14 13.89-3.79 24.75-14.84 28.47-28.98 7.48-28.4 5.54-24.97 25.95-45.75 10.17-10.35 14.14-25.44 10.42-39.58-7.47-28.38-7.48-24.42 0-52.83 3.72-14.14-.25-29.23-10.42-39.58-20.41-20.78-18.47-17.36-25.95-45.75-3.72-14.14-14.58-25.19-28.47-28.98-27.88-7.61-24.52-5.62-44.95-26.41-10.17-10.35-25-14.4-38.89-10.61-27.87 7.6-23.98 7.61-51.9 0-13.89-3.79-28.72.25-38.89 10.61-20.41 20.78-17.05 18.8-44.94 26.41-13.89 3.79-24.75 14.84-28.47 28.98-7.47 28.39-5.54 24.97-25.95 45.75-10.17 10.35-14.15 25.44-10.42 39.58 7.47 28.36 7.48 24.4 0 52.82-3.72 14.14.25 29.23 10.42 39.59 20.41 20.78 18.47 17.35 25.95 45.75 3.72 14.14 14.58 25.19 28.47 28.98C104.6 325.96 106.27 325 121 340c13.23 13.47 33.84 15.88 49.74 5.82a39.676 39.676 0 0 1 42.53 0c15.89 10.06 36.5 7.65 49.73-5.82zM97.66 175.96c0-53.03 42.24-96.02 94.34-96.02s94.34 42.99 94.34 96.02-42.24 96.02-94.34 96.02-94.34-42.99-94.34-96.02z"></path></svg> Oral Presentation</span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu24_gazemotion" style="display:none;">
We present GazeMotion – a novel method for human motion forecasting that combines information on past human poses with human eye gaze. Inspired by evidence from behavioural sciences showing that human eye and body movements are closely coordinated, GazeMotion first predicts future eye gaze from past gaze, then fuses predicted future gaze and past poses into a gaze-pose graph, and finally uses a residual graph convolutional network to forecast body motion. We extensively evaluate our method on the MoGaze, ADT, and GIMO benchmark datasets and show that it outperforms state-of-the-art methods by up to 7.4% improvement in mean per joint position error. Using head direction as a proxy to gaze, our method still achieves an average improvement of 5.5%. We finally report an online user study showing that our method also outperforms prior methods in terms of perceived realism. These results show the significant information content available in eye gaze for human motion forecasting as well as the effectiveness of our method in exploiting this information.
</div>
<div class="pub_links bg_grey" id="pub_links_hu24_gazemotion" style="display:none;">
<div class="pub_links">
  <!--<i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2021.3138902" rel="nofollow" target="_blank">doi</a></p>-->
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu24_gazemotion/pdf/hu24_gazemotion.pdf">paper.pdf</a></p>
	
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu24_gazemotion" style="display:none;">@inproceedings{hu24gazemotion,
	title={GazeMotion: Gaze-guided Human Motion Forecasting},
	author={Hu, Zhiming and Schmitt, Syn and Haeufle, Daniel and Bulling, Andreas},
	booktitle={Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems},	
	year={2024},
	pages={13017--13022}}
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="zhang24_mouse2vec" class="margin-bottom-30">
        <a href="./zhang24_mouse2vec.html">
        <img class="thumbnail" src="./zhang24_mouse2vec/image/thumb.png" title="Mouse2Vec: Learning Reusable Semantic Representations of Mouse Behaviour" alt="Mouse2Vec: Learning Reusable Semantic Representations of Mouse Behaviour">
        </a>
    
    <p class="pub_title">Mouse2Vec: Learning Reusable Semantic Representations of Mouse Behaviour</p>

    <p class="pub_author">
            <a class="a-int" href="https://www.perceptualui.org/people/gzhang/">Guanhua Zhang</a>,
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bace/">Mihai Bâce</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
    </p>
	
    <p class="pub_additional">
        
		<span class="pub_additional_journal">Proceedings of the ACM CHI Conference on Human Factors in Computing Systems (CHI), 2024: 1-17.			
		</span>
    </p>

    <p class="pub_tags">
        
        <span class="button bg_greydark"><a id="pub_abstract_sh_zhang24_mouse2vec" class="pub_show" onclick="pub_showhide('zhang24_mouse2vec','pub_abstract')">Abstract</a></span>
                
        <span class="button bg_greydark"><a id="pub_links_sh_zhang24_mouse2vec" class="pub_show" onclick="pub_showhide('zhang24_mouse2vec','pub_links')">Links</a></span>
        
        <span class="button bg_greydark"><a id="pub_bibtex_sh_zhang24_mouse2vec" class="pub_show" onclick="pub_showhide('zhang24_mouse2vec','pub_bibtex')">BibTeX</a></span>

        <span class="button bg_greydark margin-left-10"><a href="./zhang24_mouse2vec.html">Project</a></span>
        
    </p>
	
    <div class="pub_abstract bg_grey" id="pub_abstract_zhang24_mouse2vec" style="display:none;">
        The mouse is a pervasive input device used for a wide range of interactive applications. However, computational modelling of mouse behaviour typically requires time-consuming design and extraction of handcrafted features, or approaches that are application-specific. We instead propose Mouse2Vec – a novel self-supervised method designed to learn semantic representations of mouse behaviour that are reusable across users and applications. Mouse2Vec uses a Transformer-based encoder-decoder architecture, which is specifically geared for mouse data: During pretraining, the encoder learns an embedding of input mouse trajectories while the decoder reconstructs the input and simultaneously detects mouse click events. We show that the representations learned by our method can identify interpretable mouse behaviour clusters and retrieve similar mouse trajectories. We also demonstrate on three sample downstream tasks that the representations can be practically used to augment mouse data for training supervised methods and serve as an effective feature extractor.
    </div>

    <div class="pub_links bg_grey" id="pub_links_zhang24_mouse2vec" style="display:none;">
        <div class="pub_links">                            
	<i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./zhang24_mouse2vec/pdf/zhang24_mouse2vec.pdf">paper.pdf</a></p>                               
        </div>
    </div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_zhang24_mouse2vec" style="display:none;">@inproceedings{zhang24mouse2vec,
	title = {Mouse2Vec: Learning Reusable Semantic Representations of Mouse Behaviour},
	author = {Zhang, Guanhua and Hu, Zhiming and B{\^a}ce, Mihai and Bulling, Andreas},
	year = {2024},
	pages = {1--17},
	booktitle = {Proceedings of the ACM CHI Conference on Human Factors in Computing Systems},
	doi = {10.1145/3613904.3642141}}
</div>

</div>
</li></ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="wang24_salchartqa" class="margin-bottom-30">
        <a href="./wang24_salchartqa.html">
        <img class="thumbnail" src="./wang24_salchartqa/image/thumb.png" title="SalChartQA: Question-driven Saliency on Information Visualisations" alt="SalChartQA: Question-driven Saliency on Information Visualisations">
        </a>
    
    <p class="pub_title">SalChartQA: Question-driven Saliency on Information Visualisations</p>

    <p class="pub_author">
            <a class="a-int" href="https://www.perceptualui.org/people/wang/">Yao Wang</a>,
			Weitian Wang,
			Abdullah Abdelhafez,
			<a class="a-int" href="https://www.perceptualui.org/people/elfares/">Mayar Elfares</a>,
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bace/">Mihai Bâce</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
    </p>
	
    <p class="pub_additional">
        
		<span class="pub_additional_journal">Proceedings of the ACM CHI Conference on Human Factors in Computing Systems (CHI), 2024: 1-14.			
		</span>        
    </p>

    <p class="pub_tags">
        
        <span class="button bg_greydark"><a id="pub_abstract_sh_wang24_salchartqa" class="pub_show" onclick="pub_showhide('wang24_salchartqa','pub_abstract')">Abstract</a></span>
                
        <span class="button bg_greydark"><a id="pub_links_sh_wang24_salchartqa" class="pub_show" onclick="pub_showhide('wang24_salchartqa','pub_links')">Links</a></span>
        
        <span class="button bg_greydark"><a id="pub_bibtex_sh_wang24_salchartqa" class="pub_show" onclick="pub_showhide('wang24_salchartqa','pub_bibtex')">BibTeX</a></span>

        <span class="button bg_greydark margin-left-10"><a href="./wang24_salchartqa.html">Project</a></span>
        
    </p>
	
    <div class="pub_abstract bg_grey" id="pub_abstract_wang24_salchartqa" style="display:none;">
        Understanding the link between visual attention and user’s needs when visually exploring information visualisations is under-explored due to a lack of large and diverse datasets to facilitate these analyses. To fill this gap, we introduce SalChartQA – a novel crowd-sourced dataset that uses the BubbleView interface as a proxy for human gaze and a question-answering (QA) paradigm to induce different information needs in users. SalChartQA contains 74,340 answers to 6,000 questions on 3,000 visualisations. Informed by our analyses demonstrating the tight correlation between the question and visual saliency, we propose the first computational method to predict question-driven saliency on information visualisations. Our method outperforms state-of-the-art saliency models, improving several metrics, such as the correlation coefficient and the Kullback-Leibler divergence. These results show the importance of information needs for shaping attention behaviour and paving the way for new applications, such as task-driven optimisation of visualisations or explainable AI in chart question-answering.
    </div>

    <div class="pub_links bg_grey" id="pub_links_wang24_salchartqa" style="display:none;">
        <div class="pub_links">                            
	<i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./wang24_salchartqa/pdf/wang24_salchartqa.pdf">paper.pdf</a></p>                               
        </div>
    </div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_wang24_salchartqa" style="display:none;">@inproceedings{wang24salchartqa,
	title = {SalChartQA: Question-driven Saliency on Information Visualisations},
	author = {Wang, Yao and Wang, Weitian and Abdelhafez, Abdullah and Elfares, Mayar and Hu, Zhiming and B{\^a}ce, Mihai and Bulling, Andreas},
	year = {2024},
	pages = {1--14},
	booktitle = {Proceedings of the ACM CHI Conference on Human Factors in Computing Systems},
	doi = {10.1145/3613904.3642942}}
</div>

</div>
</li></ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="jiao23_supreyes" class="margin-bottom-30">    
        <a href="./jiao23_supreyes.html">
        <img class="thumbnail" src="./jiao23_supreyes/image/thumb.png" title="SUPREYES: SUPer Resolution for EYES Using Implicit Neural Representation Learning" alt="SUPREYES: SUPer Resolution for EYES Using Implicit Neural Representation Learning">
        </a>
    
    <p class="pub_title">SUPREYES: SUPer Resolution for EYES Using Implicit Neural Representation Learning</p>

    <p class="pub_author">
        
            <a class="a-int" href="https://www.perceptualui.org/people/jiao/">Chuhan Jiao</a>,
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bace/">Mihai Bâce</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
    </p>
	
    <p class="pub_additional">
        
		<span class="pub_additional_journal">Proceedings of the ACM Symposium on User Interface Software and Technology (UIST), 2023: 1-13.
		</span>				
    </p>

    <p class="pub_tags">
        
        <span class="button bg_greydark"><a id="pub_abstract_sh_jiao23_supreyes" class="pub_show" onclick="pub_showhide('jiao23_supreyes','pub_abstract')">Abstract</a></span>
                
        <span class="button bg_greydark"><a id="pub_links_sh_jiao23_supreyes" class="pub_show" onclick="pub_showhide('jiao23_supreyes','pub_links')">Links</a></span>
        
        <span class="button bg_greydark"><a id="pub_bibtex_sh_jiao23_supreyes" class="pub_show" onclick="pub_showhide('jiao23_supreyes','pub_bibtex')">BibTeX</a></span>

        <span class="button bg_greydark margin-left-10"><a href="./jiao23_supreyes.html">Project</a></span>
        
    </p>

    <div class="pub_abstract bg_grey" id="pub_abstract_jiao23_supreyes" style="display:none;">
        We introduce SUPREYES – a novel self-supervised method to increase the spatio-temporal resolution of gaze data recorded using low(er)-resolution eye trackers. Despite continuing advances in eye tracking technology, the vast majority of current eye trackers – particularly mobile ones and those integrated into mobile devices – suffer from low-resolution gaze data, thus fundamentally limiting their practical usefulness. SUPREYES learns a continuous implicit neural representation from low-resolution gaze data to up-sample the gaze data to arbitrary resolutions. We compare our method with commonly used interpolation methods on arbitrary scale super-resolution and demonstrate that SUPREYES outperforms these baselines by a significant margin. We also test on the sample downstream task of gaze-based user identification and show that our method improves the performance of original low-resolution gaze data and outperforms other baselines. These results are promising as they open up a new direction for increasing eye tracking fidelity as well as enabling new gaze-based applications without the need for new eye tracking equipment.
    </div>

    <div class="pub_links bg_grey" id="pub_links_jiao23_supreyes" style="display:none;">
        <div class="pub_links">                            
	<i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./jiao23_supreyes/pdf/jiao23_supreyes.pdf">paper.pdf</a></p>                               
        </div>
    </div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_jiao23_supreyes" style="display:none;">@inproceedings{jiao23supreyes,
	author = {Jiao, Chuhan and Hu, Zhiming and B{\^a}ce, Mihai and Bulling, Andreas},
	title = {SUPREYES: SUPer Resolution for EYES Using Implicit Neural Representation Learning},
	booktitle = {Proceedings of the ACM Symposium on User Interface Software and Technology},
	year = {2023},
	pages = {1--13},
	doi = {10.1145/3586183.3606780}}
</div>

</div>
</li></ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="zhang23_exploring" class="margin-bottom-30">    
        <a href="./zhang23_exploring.html">
        <img class="thumbnail" src="./zhang23_exploring/image/thumb.png" title="Exploring Natural Language Processing Methods for Interactive Behaviour Modelling" alt="Exploring Natural Language Processing Methods for Interactive Behaviour Modelling">
        </a>
    

    <p class="pub_title">Exploring Natural Language Processing Methods for Interactive Behaviour Modelling</p>

    <p class="pub_author">
        
            <a class="a-int" href="https://www.perceptualui.org/people/gzhang/">Guanhua Zhang</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bortoletto/">Matteo Bortoletto</a>,
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu*</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/shi/">Lei Shi</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bace/">Mihai Bâce</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
    </p>

    <p class="pub_additional">
        
		<span class="pub_additional_journal">Proceedings of the IFIP Conference on Human-Computer Interaction (INTERACT), 2023: 3-26.
		</span>			
    </p>

    <p class="pub_tags">
        
        <span class="button bg_greydark"><a id="pub_abstract_sh_zhang23_exploring" class="pub_show" onclick="pub_showhide('zhang23_exploring','pub_abstract')">Abstract</a></span>
        

        
        <span class="button bg_greydark"><a id="pub_links_sh_zhang23_exploring" class="pub_show" onclick="pub_showhide('zhang23_exploring','pub_links')">Links</a></span>
        

        <span class="button bg_greydark"><a id="pub_bibtex_sh_zhang23_exploring" class="pub_show" onclick="pub_showhide('zhang23_exploring','pub_bibtex')">BibTeX</a></span>

        <span class="button bg_greydark margin-left-10"><a href="./zhang23_exploring.html">Project</a></span>
        <span class="pub_award"><svg class="svg-inline--fa fa-award fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="award" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M97.12 362.63c-8.69-8.69-4.16-6.24-25.12-11.85-9.51-2.55-17.87-7.45-25.43-13.32L1.2 448.7c-4.39 10.77 3.81 22.47 15.43 22.03l52.69-2.01L105.56 507c8 8.44 22.04 5.81 26.43-4.96l52.05-127.62c-10.84 6.04-22.87 9.58-35.31 9.58-19.5 0-37.82-7.59-51.61-21.37zM382.8 448.7l-45.37-111.24c-7.56 5.88-15.92 10.77-25.43 13.32-21.07 5.64-16.45 3.18-25.12 11.85-13.79 13.78-32.12 21.37-51.62 21.37-12.44 0-24.47-3.55-35.31-9.58L252 502.04c4.39 10.77 18.44 13.4 26.43 4.96l36.25-38.28 52.69 2.01c11.62.44 19.82-11.27 15.43-22.03zM263 340c15.28-15.55 17.03-14.21 38.79-20.14 13.89-3.79 24.75-14.84 28.47-28.98 7.48-28.4 5.54-24.97 25.95-45.75 10.17-10.35 14.14-25.44 10.42-39.58-7.47-28.38-7.48-24.42 0-52.83 3.72-14.14-.25-29.23-10.42-39.58-20.41-20.78-18.47-17.36-25.95-45.75-3.72-14.14-14.58-25.19-28.47-28.98-27.88-7.61-24.52-5.62-44.95-26.41-10.17-10.35-25-14.4-38.89-10.61-27.87 7.6-23.98 7.61-51.9 0-13.89-3.79-28.72.25-38.89 10.61-20.41 20.78-17.05 18.8-44.94 26.41-13.89 3.79-24.75 14.84-28.47 28.98-7.47 28.39-5.54 24.97-25.95 45.75-10.17 10.35-14.15 25.44-10.42 39.58 7.47 28.36 7.48 24.4 0 52.82-3.72 14.14.25 29.23 10.42 39.59 20.41 20.78 18.47 17.35 25.95 45.75 3.72 14.14 14.58 25.19 28.47 28.98C104.6 325.96 106.27 325 121 340c13.23 13.47 33.84 15.88 49.74 5.82a39.676 39.676 0 0 1 42.53 0c15.89 10.06 36.5 7.65 49.73-5.82zM97.66 175.96c0-53.03 42.24-96.02 94.34-96.02s94.34 42.99 94.34 96.02-42.24 96.02-94.34 96.02-94.34-42.99-94.34-96.02z"></path></svg> Best Student Paper Nominees</span>
    </p>

    <div class="pub_abstract bg_grey" id="pub_abstract_zhang23_exploring" style="display:none;">
        Analysing and modelling interactive behaviour is an important topic in human-computer interaction (HCI) and a key requirement for the development of intelligent interactive systems. Interactive behaviour has a sequential (actions happen one after another) and hierarchical (a sequence of actions forms an activity driven by interaction goals) structure, which may be similar to the structure of natural language. Designed based on such a structure, natural language processing (NLP) methods have achieved groundbreaking success in various downstream tasks. However, few works linked interactive behaviour with natural language. In this paper, we explore the similarity between interactive behaviour and natural language by applying an NLP method, byte pair encoding (BPE), to encode mouse and keyboard behaviour. We then analyse the vocabulary, i.e., the set of action sequences, learnt by BPE, as well as use the vocabulary to encode the input behaviour for interactive task recognition. An existing dataset collected in constrained lab settings and our novel out-of-the-lab dataset were used for evaluation. Results show that this natural language-inspired approach not only learns action sequences that reflect specific interaction goals, but also achieves higher F1 scores on task recognition than other methods. Our work reveals the similarity between interactive behaviour and natural language, and presents the potential of applying the new pack of methods that leverage insights from NLP to model interactive behaviour in HCI.
    </div>

    <div class="pub_links bg_grey" id="pub_links_zhang23_exploring" style="display:none;">
        <div class="pub_links">
                
                
                    <i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./zhang23_exploring/pdf/zhang23_exploring.pdf">paper.pdf</a></p>
                
                
        </div>
    </div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_zhang23_exploring" style="display:none;">@inproceedings{zhang23exploring,
	title = {Exploring Natural Language Processing Methods for Interactive Behaviour Modelling},
	author = {Zhang, Guanhua and Bortoletto, Matteo and Hu, Zhiming and Shi, Lei and B{\^a}ce, Mihai and Bulling, Andreas},
	booktitle = {Proceedings of the IFIP Conference on Human-Computer Interaction},
	pages = {3--26},
	year = {2023},
	publisher = {Springer}}
</div>

</div>
</li></ol>


<br><h4>Short Papers, Abstracts, and Workshops</h4>
<!--paper-->
<ol class="bibliography"><li>
<div id="elfares22_federated" class="margin-bottom-30">
       
	<a href="./elfares22_federated.html">
	<img class="thumbnail" src="./elfares22_federated/image/thumb.png" title="Federated Learning for Appearance-based Gaze Estimation in the Wild" alt="Federated Learning for Appearance-based Gaze Estimation in the Wild">
	</a>
    

    <p class="pub_title">Federated Learning for Appearance-based Gaze Estimation in the Wild</p>

    <p class="pub_author">
        
            <a class="a-int" href="https://www.perceptualui.org/people/elfares/">Mayar Elfares</a>,
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
            Pascal Reisert,
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>,
            Ralf Küsters
    </p>

    <p class="pub_additional">
        
		<span class="pub_additional_journal">Proceedings of the NeurIPS Workshop Gaze Meets ML (NeurIPS GMML), 2023: 20-36.</span>            
    </p>

    <p class="pub_tags">
        
        <span class="button bg_greydark"><a id="pub_abstract_sh_elfares22_federated" class="pub_show" onclick="pub_showhide('elfares22_federated','pub_abstract')">Abstract</a></span>
        

        
        <span class="button bg_greydark"><a id="pub_links_sh_elfares22_federated" class="pub_show" onclick="pub_showhide('elfares22_federated','pub_links')">Links</a></span>
        

        <span class="button bg_greydark"><a id="pub_bibtex_sh_elfares22_federated" class="pub_show" onclick="pub_showhide('elfares22_federated','pub_bibtex')">BibTeX</a></span>

        <span class="button bg_greydark margin-left-10"><a href="./elfares22_federated.html">Project</a></span>

        
    </p>

    <div class="pub_abstract bg_grey" id="pub_abstract_elfares22_federated" style="display:none;">
        Gaze estimation methods have significantly matured in recent years but the large number of eye images required to train deep learning models poses significant privacy risks. In addition, the heterogeneous data distribution across different users can significantly hinder the training process. In this work, we propose the first federated learning approach for gaze estimation to preserve the privacy of gaze data. We further employ pseudo-gradients optimisation to adapt our federated learning approach to the divergent model updates to address the heterogeneous nature of in-the-wild gaze data in collaborative setups. We evaluate our approach on a real-world dataset (MPIIGaze dataset) and show that our work enhances the privacy guarantees of conventional appearance-based gaze estimation methods, handles the convergence issues of gaze estimators, and significantly outperforms vanilla federated learning by 15.8% (from a mean error of 10.63 degrees to 8.95 degrees). As such, our work paves the way to develop privacy-aware collaborative 14 learning setups for gaze estimation while maintaining the model’s performance.
    </div>

    <div class="pub_links bg_grey" id="pub_links_elfares22_federated" style="display:none;">
        <div class="pub_links">
              
                
                    <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.48550/arXiv.2211.07330" rel="nofollow" target="_blank">doi</a></p>

                    <i class="fa fa-file-pdf"></i><p>Paper: <a class="pub_list" href="./elfares22_federated/pdf/elfares22_federated.pdf">paper.pdf</a></p>
                
                
                
        </div>
    </div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_elfares22_federated" style="display:none;">@inproceedings{elfares22federated,
	title = {Federated Learning for Appearance-based Gaze Estimation in the Wild},
	author = {Elfares, Mayar and Hu, Zhiming and Reisert, Pascal and Bulling, Andreas and Küsters, Ralf},
	year = {2023},
	booktitle = {Proceedings of the NeurIPS Workshop Gaze Meets ML},
	pages = {20--36}}
</div>
</div>
</li></ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu21_eye" class="margin-bottom-30">
<a href="./hu21_eye.html">
<img class="thumbnail" src="./hu21_eye/image/thumb.png" title="Eye Fixation Forecasting in Task-Oriented Virtual Reality" alt="Eye Fixation Forecasting in Task-Oriented Virtual Reality">
</a>
<p class="pub_title">Eye Fixation Forecasting in Task-Oriented Virtual Reality</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">Proceedings of the IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), 2021: 707-708.
</span>
</p>
<p class="pub_tags">        
<span class="button bg_greydark"><a id="pub_abstract_sh_hu21_eye" class="pub_show" onclick="pub_showhide(&#39;hu21_eye&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
<span class="button bg_greydark"><a id="pub_links_sh_hu21_eye" class="pub_show" onclick="pub_showhide(&#39;hu21_eye&#39;,&#39;pub_links&#39;)">Links</a></span>
<span class="button bg_greydark"><a id="pub_bibtex_sh_hu21_eye" class="pub_show" onclick="pub_showhide(&#39;hu21_eye&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
<span class="button bg_greydark margin-left-10"><a href="./hu21_eye.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu21_eye" style="display:none;">
In immersive virtual reality (VR), users' visual attention is crucial for many important applications, including VR content design, gaze-based interaction, and gaze-contingent rendering. 
Especially, information on users' future eye fixations is key for intelligent user interfaces and has significant relevance for many areas, such as visual attention enhancement, dynamic event triggering, and human-computer interaction. 
However, previous works typically focused on free-viewing conditions and paid less attention to task-oriented attention. 
This paper aims at forecasting users' eye fixations in task-oriented virtual reality. 
To this end, a VR eye tracking dataset that corresponds to different users performing a visual search task in immersive virtual environments is built. 
A comprehensive analysis of users' eye fixations is performed based on the collected data. 
The analysis reveals that eye fixations are correlated with users' historical gaze positions, task-related objects, saliency information of the VR content, and head rotation velocities. 
Based on this analysis, a novel learning-based model is proposed to forecast users' eye fixations in the near future in immersive virtual environments.</div>
<div class="pub_links bg_grey" id="pub_links_hu21_eye" style="display:none;">
<div class="pub_links">            
	<svg class="svg-inline--fa fa-fingerprint fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="fingerprint" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M256.12 245.96c-13.25 0-24 10.74-24 24 1.14 72.25-8.14 141.9-27.7 211.55-2.73 9.72 2.15 30.49 23.12 30.49 10.48 0 20.11-6.92 23.09-17.52 13.53-47.91 31.04-125.41 29.48-224.52.01-13.25-10.73-24-23.99-24zm-.86-81.73C194 164.16 151.25 211.3 152.1 265.32c.75 47.94-3.75 95.91-13.37 142.55-2.69 12.98 5.67 25.69 18.64 28.36 13.05 2.67 25.67-5.66 28.36-18.64 10.34-50.09 15.17-101.58 14.37-153.02-.41-25.95 19.92-52.49 54.45-52.34 31.31.47 57.15 25.34 57.62 55.47.77 48.05-2.81 96.33-10.61 143.55-2.17 13.06 6.69 25.42 19.76 27.58 19.97 3.33 26.81-15.1 27.58-19.77 8.28-50.03 12.06-101.21 11.27-152.11-.88-55.8-47.94-101.88-104.91-102.72zm-110.69-19.78c-10.3-8.34-25.37-6.8-33.76 3.48-25.62 31.5-39.39 71.28-38.75 112 .59 37.58-2.47 75.27-9.11 112.05-2.34 13.05 6.31 25.53 19.36 27.89 20.11 3.5 27.07-14.81 27.89-19.36 7.19-39.84 10.5-80.66 9.86-121.33-.47-29.88 9.2-57.88 28-80.97 8.35-10.28 6.79-25.39-3.49-33.76zm109.47-62.33c-15.41-.41-30.87 1.44-45.78 4.97-12.89 3.06-20.87 15.98-17.83 28.89 3.06 12.89 16 20.83 28.89 17.83 11.05-2.61 22.47-3.77 34-3.69 75.43 1.13 137.73 61.5 138.88 134.58.59 37.88-1.28 76.11-5.58 113.63-1.5 13.17 7.95 25.08 21.11 26.58 16.72 1.95 25.51-11.88 26.58-21.11a929.06 929.06 0 0 0 5.89-119.85c-1.56-98.75-85.07-180.33-186.16-181.83zm252.07 121.45c-2.86-12.92-15.51-21.2-28.61-18.27-12.94 2.86-21.12 15.66-18.26 28.61 4.71 21.41 4.91 37.41 4.7 61.6-.11 13.27 10.55 24.09 23.8 24.2h.2c13.17 0 23.89-10.61 24-23.8.18-22.18.4-44.11-5.83-72.34zm-40.12-90.72C417.29 43.46 337.6 1.29 252.81.02 183.02-.82 118.47 24.91 70.46 72.94 24.09 119.37-.9 181.04.14 246.65l-.12 21.47c-.39 13.25 10.03 24.31 23.28 24.69.23.02.48.02.72.02 12.92 0 23.59-10.3 23.97-23.3l.16-23.64c-.83-52.5 19.16-101.86 56.28-139 38.76-38.8 91.34-59.67 147.68-58.86 69.45 1.03 134.73 35.56 174.62 92.39 7.61 10.86 22.56 13.45 33.42 5.86 10.84-7.62 13.46-22.59 5.84-33.43z"></path></svg><p>Doi: <a class="a-text-ext" href="https://doi.org/10.1109/VRW52623.2021.00236" rel="nofollow" target="_blank">doi</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu21_eye/pdf/hu21_eye.pdf">paper.pdf</a></p>                
  <i class="fa fa-file-powerpoint"></i>&nbsp;&nbsp; Slides: <a class="pub_list" href="./hu21_eye/ppt/hu21_eye.pdf">slides.pdf</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu21_eye" style="display:none;">@inproceedings{hu21eye, 
	author={Hu, Zhiming},
	title = {Eye Fixation Forecasting in Task-Oriented Virtual Reality}, 
	booktitle={Proceedings of the IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops},
	year = {2021},
	pages={707-708},
	organization={IEEE}} 
</div>
</ol>


<!--paper-->
<ol class="bibliography"><li>
<div id="hu20_gaze" class="margin-bottom-30">
<a href="./hu20_gaze.html">
<img class="thumbnail" src="./hu20_gaze/image/thumb.png" title="Gaze Analysis and Prediction in Virtual Reality" alt="Gaze Analysis and Prediction in Virtual Reality">
</a>
<p class="pub_title">Gaze Analysis and Prediction in Virtual Reality</p>
<p class="pub_author">
<a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>
</p>
<p class="pub_additional">        
<span class="pub_additional_journal">Proceedings of the IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW), 2020: 543-544.
</span>
</p>
<p class="pub_tags">        
<span class="button bg_greydark"><a id="pub_abstract_sh_hu20_gaze" class="pub_show" onclick="pub_showhide(&#39;hu20_gaze&#39;,&#39;pub_abstract&#39;)">Abstract</a></span>
<span class="button bg_greydark"><a id="pub_links_sh_hu20_gaze" class="pub_show" onclick="pub_showhide(&#39;hu20_gaze&#39;,&#39;pub_links&#39;)">Links</a></span>
<span class="button bg_greydark"><a id="pub_bibtex_sh_hu20_gaze" class="pub_show" onclick="pub_showhide(&#39;hu20_gaze&#39;,&#39;pub_bibtex&#39;)">BibTeX</a></span>
<span class="button bg_greydark margin-left-10"><a href="./hu20_gaze.html">Project</a></span>
</p>
<div class="pub_abstract bg_grey" id="pub_abstract_hu20_gaze" style="display:none;">
In virtual reality (VR) systems, users’ gaze information has gained importance in recent years. 
It can be applied to many aspects, including VR content design, eye-movement based interaction, gaze-contingent rendering, etc. 
In this context, it becomes increasingly important to understand users’ gaze behaviors in virtual reality and to predict users’ gaze positions. 
This paper presents research in gaze behavior analysis and gaze position prediction in virtual reality. 
Specifically, this paper focuses on static virtual scenes and dynamic virtual scenes under free-viewing conditions. 
Users’ gaze data in virtual scenes are collected and statistical analysis is performed on the recorded data. 
The analysis reveals that users’ gaze positions are correlated with their head rotation velocities and the salient regions of the content. 
In dynamic scenes, users’ gaze positions also have strong correlations with the positions of dynamic objects. 
A data-driven eye-head coordination model is proposed for realtime gaze prediction in static scenes and a CNN-based model is derived for predicting gaze positions in dynamic scenes.
</div>
<div class="pub_links bg_grey" id="pub_links_hu20_gaze" style="display:none;">
<div class="pub_links">            
	<svg class="svg-inline--fa fa-fingerprint fa-w-16" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="fingerprint" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" data-fa-i2svg=""><path fill="currentColor" d="M256.12 245.96c-13.25 0-24 10.74-24 24 1.14 72.25-8.14 141.9-27.7 211.55-2.73 9.72 2.15 30.49 23.12 30.49 10.48 0 20.11-6.92 23.09-17.52 13.53-47.91 31.04-125.41 29.48-224.52.01-13.25-10.73-24-23.99-24zm-.86-81.73C194 164.16 151.25 211.3 152.1 265.32c.75 47.94-3.75 95.91-13.37 142.55-2.69 12.98 5.67 25.69 18.64 28.36 13.05 2.67 25.67-5.66 28.36-18.64 10.34-50.09 15.17-101.58 14.37-153.02-.41-25.95 19.92-52.49 54.45-52.34 31.31.47 57.15 25.34 57.62 55.47.77 48.05-2.81 96.33-10.61 143.55-2.17 13.06 6.69 25.42 19.76 27.58 19.97 3.33 26.81-15.1 27.58-19.77 8.28-50.03 12.06-101.21 11.27-152.11-.88-55.8-47.94-101.88-104.91-102.72zm-110.69-19.78c-10.3-8.34-25.37-6.8-33.76 3.48-25.62 31.5-39.39 71.28-38.75 112 .59 37.58-2.47 75.27-9.11 112.05-2.34 13.05 6.31 25.53 19.36 27.89 20.11 3.5 27.07-14.81 27.89-19.36 7.19-39.84 10.5-80.66 9.86-121.33-.47-29.88 9.2-57.88 28-80.97 8.35-10.28 6.79-25.39-3.49-33.76zm109.47-62.33c-15.41-.41-30.87 1.44-45.78 4.97-12.89 3.06-20.87 15.98-17.83 28.89 3.06 12.89 16 20.83 28.89 17.83 11.05-2.61 22.47-3.77 34-3.69 75.43 1.13 137.73 61.5 138.88 134.58.59 37.88-1.28 76.11-5.58 113.63-1.5 13.17 7.95 25.08 21.11 26.58 16.72 1.95 25.51-11.88 26.58-21.11a929.06 929.06 0 0 0 5.89-119.85c-1.56-98.75-85.07-180.33-186.16-181.83zm252.07 121.45c-2.86-12.92-15.51-21.2-28.61-18.27-12.94 2.86-21.12 15.66-18.26 28.61 4.71 21.41 4.91 37.41 4.7 61.6-.11 13.27 10.55 24.09 23.8 24.2h.2c13.17 0 23.89-10.61 24-23.8.18-22.18.4-44.11-5.83-72.34zm-40.12-90.72C417.29 43.46 337.6 1.29 252.81.02 183.02-.82 118.47 24.91 70.46 72.94 24.09 119.37-.9 181.04.14 246.65l-.12 21.47c-.39 13.25 10.03 24.31 23.28 24.69.23.02.48.02.72.02 12.92 0 23.59-10.3 23.97-23.3l.16-23.64c-.83-52.5 19.16-101.86 56.28-139 38.76-38.8 91.34-59.67 147.68-58.86 69.45 1.03 134.73 35.56 174.62 92.39 7.61 10.86 22.56 13.45 33.42 5.86 10.84-7.62 13.46-22.59 5.84-33.43z"></path></svg><p>Doi: <a class="a-text-ext" href="https://doi.org/10.1109/vrw50115.2020.00123" rel="nofollow" target="_blank">doi</a></p>
	<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu20_gaze/pdf/hu20_gaze.pdf">paper.pdf</a></p>                
  <i class="fa fa-file-powerpoint"></i>&nbsp; Slides: <a class="pub_list" href="./hu20_gaze/ppt/hu20_gaze.pdf">slides.pdf</a></p>
</div>
</div>
<div class="pub_bibtex bg_grey" id="pub_bibtex_hu20_gaze" style="display:none;">@inproceedings{hu20gaze, 
	author={Hu, Zhiming},
	title = {Gaze Analysis and Prediction in Virtual Reality}, 
	booktitle={Proceedings of the IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops},
	year = {2020},
	pages={543--544},
	organization={IEEE}} 
</div>
</ol>


<hr>
<h3>Personal Blogs</h3>
All blogs are written in Chinese.
<ul>
<li> <a class="a-text-ext" href= "https://mp.weixin.qq.com/s/eKz_7gbORipYApq49Vs9Ew?token=1655051930" title=""> 写在三十岁之前，与精神内耗再见</a>, 2025</li>
<li> <a class="a-text-ext" href= "https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483828&idx=1&sn=d1ca10341d3e6f1a3141db56f252af80&chksm=ebbedf77dcc95661aefd8927569374a341301c18290f9d9c814d95720b907ce84f228b66b130&scene=132&exptype=timeline_recommend_article_extendread_samebiz#wechat_redirect" title=""> 理性与偏见</a>, 2023</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483824&idx=1&sn=5f2b32fca7e147844de53301ac197f61&chksm=ebbedf73dcc956654ac7fb23046e7731568bd92f513495110f0fe37a708fe443324e86b190b8&token=781182399&lang=zh_CN#rd" title=""> 聊一聊“卷”和“同辈压力”
</a>, 2023</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483818&idx=1&sn=8fc606f7a13ee74c3fbea2c0399d5cbd&chksm=ebbedf69dcc9567fc80a8edb0ee86509c1d3aeba6ab13f065023ac2c5078a5c4b7e80f171ce5&token=781182399&lang=zh_CN#rd" title=""> 从博士到博士后</a>, 2022</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483762&idx=1&sn=11b9a80092adfc95abb31755b6e91d4a&chksm=ebbedfb1dcc956a7a3fa8d427978280816689802f28f6e0492fdde811bf8c0663a12afa96127&token=781182399&lang=zh_CN#rd" title=""> 写在博士生涯的最后</a>, 2022</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483752&idx=1&sn=9eea860e76a73516e5ee694e909c9142&chksm=ebbedfabdcc956bd473ce16ec52f29235d62334192a583358f7c330d544038a3926512256b26&token=781182399&lang=zh_CN#rd" title=""> 学了三个专业之后</a>, 2021</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483736&idx=1&sn=a6f827edf5156e82bcb571198372772b&chksm=ebbedf9bdcc9568d8e026b752d0e1f140c5803f67f9b94b74a879c0207371e366e0840a6a599&token=781182399&lang=zh_CN#rd" title=""> Research，还真是有趣呢~</a>, 2020</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483720&idx=1&sn=267485316bb6990035348ea618b6ec26&chksm=ebbedf8bdcc9569d14cbf7ddf665e887151e25128b96a099b692f0bd4240fe9dac3d3d0b2a0c&token=781182399&lang=zh_CN#rd" title=""> 科研路上的一颗糖</a>, 2019</li>
<li> <a class="a-text-ext" href="https://mp.weixin.qq.com/s?__biz=MzI3OTk3Mjc4MA==&mid=2247483701&idx=1&sn=bf87eda79bb28dcf6680196a18e14c24&chksm=ebbedff6dcc956e0e7921946011b0a2acfc34e357889d97e5b62aa6cfbc0743e31c09c285173&token=781182399&lang=zh_CN#rd" title=""> 博士第一年：有发堪学直须学</a>, 2018</li>
</ul>

</dl>
</dl>
</dl>
</dl>
</dl>
</div>
</div>

</div>

<!-- footer -->
<div id="footer" class="footer-v1">
<div class="copyright custom-copyright">
<div class="container">
<div class="row">
<div class="col-md-6">
<p>
<span class="custom-copyright-container">
	Last modified: 13/08/2025
</span>
</p>
</div>
</div>
</div>
</div>
</div>

</body></html>