<!DOCTYPE html>
<html lang="en">
    <meta http-equiv="content-type" content="text/html;charset=utf-8" />
    <head>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
        <title>HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended Reality Exploiting Eye-Hand-Head Coordination</title>
        <link rel="stylesheet" media="all" href="./index/css/main_v2.css"/>
        <link rel="stylesheet" media="all" href="./hu25_hoigaze/css/owl.carousel.min.css" />
        <link rel="stylesheet" media="all" href="./hu25_hoigaze/css/owl.theme.default.min.css" />
        <link rel="stylesheet" media="all" href="./hu25_hoigaze/css/jquery-ui.min.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
        <script src="./hu25_hoigaze/js/jquery-3.4.1.min.js"></script>
        <script src="./hu25_hoigaze/js/jquery-ui.min.js"></script>
        <script src="./hu25_hoigaze/js/fontawesome-5.11.2.js"></script>
        <script src="./hu25_hoigaze/js/main.js"></script>
        <script src="./hu25_hoigaze/js/owl.carousel.min.js"></script>
        <script src="./hu25_hoigaze/js/rot13.js"></script>
		<script src="https://www.w3counter.com/tracker.js?id=150008"></script>
    </head>
    <body class="header header-location">
        <div class="wrapper">
            <!-- header -->
            <div id="unstickyheader" class="unstickyheader">
                <div class="topbar">
                    <div class="container" >
                        
                    </div>
                </div>
            </div>
        </div>
        

        <!-- content -->
        <div class="content">
        <div class="section margin-top-20 margin-bottom-40">
    <div class="container">

        <h3>HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended Reality Exploiting Eye-Hand-Head Coordination</h3>

        <h4>
        
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>, <a class="a-int" href="https://scholar.google.de/citations?user=HO3nVAoAAAAJ&hl=de">Daniel Haeufle</a>, <a class="a-int" href="https://www.imsb.uni-stuttgart.de/team/Schmitt-00006/">Syn Schmitt</a>, <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
        </h4>

        <h4>
        
            <span class="pub_additional_journal">Proceedings of the ACM Special Interest Group on Computer Graphics and Interactive Techniques (SIGGRAPH), 2025: 1-10.
			</span>    
        </h4>

		<hr>
		<img src="./hu25_hoigaze/image/teaser.png" style="height:auto; width:1024px;" class="centerContent"><br>
		<i class="centerContent"></i>            
        <hr>
                

	<h4>Abstract</h4>
	We present HOIGaze – a novel learning-based approach for gaze estimation during hand-object interactions (HOI) in extended reality (XR). HOIGaze addresses the challenging HOI setting by building on one key insight: The eye, hand, and head movements are closely coordinated during HOIs and this coordination can be exploited to identify samples that are most useful for gaze estimator training – as such, effectively denoising the training data. This denoising approach is in stark contrast to previous gaze estimation methods that treated all training samples as equal. Specifically, we propose: 1) a novel hierarchical framework that first recognises the hand currently visually attended to and then estimates gaze direction based on the attended hand; 2) a new gaze estimator that uses cross-modal Transformers to fuse head and hand-object features extracted using a convolutional neural network and a spatio-temporal graph convolutional network; and 3) a novel eye-head coordination loss that upgrades training samples belonging to the coordinated eye-head movements. We evaluate HOIGaze on the HOT3D and Aria digital twin (ADT) datasets and show that it significantly outperforms state-of-the-art methods, achieving an average improvement of 15.6% on HOT3D and 6.0% on ADT in mean angular error. To demonstrate the potential of our method, we further report significant performance improvements for the sample downstream task of eye-based activity recognition on ADT. Taken together, our results underline the significant information content available in eye-hand-head coordination and, as such, open up an exciting new direction for learning-based gaze estimation.
	<hr>

	<h4>Presentation Video</h4>
	<object type='application/x-shockwave-flash' style='width:1024px; height:576px;' data='https://www.youtube.com/v/L4ipTlBIVms' class="centerContent">
	<param name='movie' value='https://www.youtube.com/v/L4ipTlBIVms'/>
	</object>        
	<br>
	
	<h4>Demo Video</h4>
	<object type='application/x-shockwave-flash' style='width:1024px; height:576px;' data='https://www.youtube.com/v/FKGm_aag2R8' class="centerContent">
	<param name='movie' value='https://www.youtube.com/v/FKGm_aag2R8'/>
	</object>        
	<hr>
	
	<h4>Links</h4>
	<div class="pub_links">
					
		<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu25_hoigaze/pdf/hu25_hoigaze.pdf">paper.pdf</a></p>
		<i class="fa fa-file-pdf"></i>&nbsp;Supplementary materials: <a class="pub_list" href="./hu25_hoigaze/pdf/hu25_hoigaze_supplementary_material.pdf">supplementary_material.pdf</a></p>
		<!--<i class="fa fa-file-powerpoint"></i>&nbsp; Slides: <a class="pub_list" href="./hu25_hoigaze/ppt/hu25_hoigaze.pdf">slides.pdf</a></p>-->
		<i class="fa fa-code"></i>&nbsp;Code: <a class="a-text-ext" href="https://github.com/CraneHzm/HOIGaze">code</a></p>
	</div>
	
	<hr>

	<h4>BibTeX</h4>

<div class="pub_bibtex bg_grey">@inproceedings{hu25hoigaze,
	title={HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended Reality Exploiting Eye-Hand-Head Coordination},
	author={Hu, Zhiming and Haeufle, Daniel and Schmitt, Syn and Bulling, Andreas},
	booktitle={Proceedings of the ACM Special Interest Group on Computer Graphics and Interactive Techniques},
	year={2025},
	pages = {1--10}}
</div>
    </div>
</div>
        </div>

        <!-- footer -->
        <div id="footer" class="footer-v1">
            
            <div class="copyright custom-copyright">
                <div class="container">
                    <div class="row">
                        <div class="col-md-6">
                            <p>
                                <span class="custom-copyright-container">
                                    Last modified: 11/08/2025
                                </span>
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        
    </body>
</html>