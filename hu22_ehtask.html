<!DOCTYPE html>
<html lang="en">
    <meta http-equiv="content-type" content="text/html;charset=utf-8" />
    <head>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
        <title>EHTask: Recognizing User Tasks from Eye and Head Movements in Immersive Virtual Reality</title>
        <link rel="stylesheet" media="all" href="./index/css/main_v2.css" />
        <link rel="stylesheet" media="all" href="./hu22_ehtask/css/owl.carousel.min.css" />
        <link rel="stylesheet" media="all" href="./hu22_ehtask/css/owl.theme.default.min.css" />
        <link rel="stylesheet" media="all" href="./hu22_ehtask/css/jquery-ui.min.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
        <script src="./hu22_ehtask/js/jquery-3.4.1.min.js"></script>
        <script src="./hu22_ehtask/js/jquery-ui.min.js"></script>
        <script src="./hu22_ehtask/js/fontawesome-5.11.2.js"></script>
        <script src="./hu22_ehtask/js/main.js"></script>
        <script src="./hu22_ehtask/js/owl.carousel.min.js"></script>
        <script src="./hu22_ehtask/js/rot13.js"></script>
		<script src="https://www.w3counter.com/tracker.js?id=150008"></script>
    </head>
    <body class="header header-location">
        <div class="wrapper">
            <!-- header -->
            <div id="unstickyheader" class="unstickyheader">
                <div class="topbar">
                    <div class="container" >
                        
                    </div>
                </div>
            </div>
        </div>
        

        <!-- content -->
        <div class="content">
        <div class="section margin-top-20 margin-bottom-40">
    <div class="container">

        <h3>EHTask: Recognizing User Tasks from Eye and Head Movements in Immersive Virtual Reality</h3>

        <h4>
        
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>,
            <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>,
            Sheng Li, Guoping Wang
        </h4>

        <h4>
			<span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics (TVCG, oral presentation at IEEE VR 2022), 2023, 29(4): 1992-2004.
			</span>            
        </h4>

		<hr>
		<img src="./hu22_ehtask/image/teaser.png" style="height:auto; width:1024px;" class="centerContent"><br>
		<i class="centerContent"></i>            
        <hr>                

        <h4>Abstract</h4>

        Understanding human visual attention in immersive virtual reality (VR) is crucial for many important applications, including gaze prediction, gaze guidance, and gaze-contingent rendering. However, previous works on visual attention analysis typically only explored one specific VR task and paid less attention to the differences between different tasks. Moreover, existing task recognition methods typically focused on 2D viewing conditions and only explored the effectiveness of human eye movements. We first collect eye and head movements of 30 participants performing four tasks, i.e. Free viewing, Visual search, Saliency, and Track, in 15 360-degree VR videos. Using this dataset, we analyze the patterns of human eye and head movements and reveal significant differences across different tasks in terms of fixation duration, saccade amplitude, head rotation velocity, and eye-head coordination. We then propose EHTask â€“ a novel learning-based method that employs eye and head movements to recognize user tasks in VR. We show that our method significantly outperforms the state-of-the-art methods derived from 2D viewing conditions both on our dataset (accuracy of 84.4% vs. 62.8%) and on a real-world dataset (61.9% vs. 44.1%). As such, our work provides meaningful insights into human visual attention under different VR tasks and guides future work on recognizing user tasks in VR.

        <hr>

        <h4>Presentation Video</h4>
        <object type='application/x-shockwave-flash' style='width:1024px; height:576px;' data='https://www.youtube.com/v/deDSo1qGSZ8' class="centerContent">
        <param name='movie' value='https://www.youtube.com/v/deDSo1qGSZ8' />
        </object>       
        <hr>

        <h4>Links</h4>
        <div class="pub_links">
            
            <i class="fa fa-fingerprint"></i><p>Doi: <a class="a-text-ext" href="http://dx.doi.org/10.1109/TVCG.2021.3138902" rel="nofollow" target="_blank">doi</a></p>
	        <svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu22_ehtask/pdf/hu22_ehtask.pdf">paper.pdf</a></p>
            <i class="fa fa-database"></i>&nbsp;&nbsp;Dataset: <a class="a-text-ext" href="https://chinapku-my.sharepoint.com/:f:/g/personal/1701111311_pku_edu_cn/Ek7OFKlI9NVGlJkB7A-_xOwBM7eU-I73kj_qDnv8cXU5Dw?e=TokN9w">dataset</a></p>			
            <i class="fa fa-file-powerpoint"></i>&nbsp; Slides: <a class="pub_list" href="./hu22_ehtask/ppt/hu22_ehtask.pdf">slides.pdf</a></p>
            <i class="fa fa-code"></i>&nbsp;Code: <a class="a-text-ext" href="https://github.com/CraneHzm/EHTask">code</a></p>
        </div>

        <hr>

        <h4>BibTeX</h4>

<div class="pub_bibtex bg_grey">@article{hu22ehtask,
	author={Hu, Zhiming and Bulling, Andreas and Li, Sheng and Wang, Guoping},
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={EHTask: Recognizing User Tasks From Eye and Head Movements in Immersive Virtual Reality}, 
	year={2023},
	volume={29},
	number={4},
	pages={1992--2004},
	doi={10.1109/TVCG.2021.3138902}}          
</div>

        
    </div>
</div>

        </div>

        <!-- footer -->
        <div id="footer" class="footer-v1">
            
            <div class="copyright custom-copyright">
                <div class="container">
                    <div class="row">
                        <div class="col-md-6">
                            <p>
                                <span class="custom-copyright-container">
                                    Last modified: 28/06/2023
                                </span>
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        
    </body>
</html>
