<!DOCTYPE html>
<html lang="en">
    <meta http-equiv="content-type" content="text/html;charset=utf-8" />
    <head>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
        <title>GazeMoDiff: Gaze-guided Diffusion Model for Stochastic Human Motion Prediction</title>
        <link rel="stylesheet" media="all" href="./index/css/main_v2.css"/>
        <link rel="stylesheet" media="all" href="./yan24_gazemodiff/css/owl.carousel.min.css" />
        <link rel="stylesheet" media="all" href="./yan24_gazemodiff/css/owl.theme.default.min.css" />
        <link rel="stylesheet" media="all" href="./yan24_gazemodiff/css/jquery-ui.min.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
        <script src="./yan24_gazemodiff/js/jquery-3.4.1.min.js"></script>
        <script src="./yan24_gazemodiff/js/jquery-ui.min.js"></script>
        <script src="./yan24_gazemodiff/js/fontawesome-5.11.2.js"></script>
        <script src="./yan24_gazemodiff/js/main.js"></script>
        <script src="./yan24_gazemodiff/js/owl.carousel.min.js"></script>
        <script src="./yan24_gazemodiff/js/rot13.js"></script>
		<script src="https://www.w3counter.com/tracker.js?id=150008"></script>
    </head>
    <body class="header header-location">
        <div class="wrapper">
            <!-- header -->
            <div id="unstickyheader" class="unstickyheader">
                <div class="topbar">
                    <div class="container" >
                        
                    </div>
                </div>
            </div>
        </div>
        
        <!-- content -->
        <div class="content">
        <div class="section margin-top-20 margin-bottom-40">
    <div class="container">

        <h3>GazeMoDiff: Gaze-guided Diffusion Model for Stochastic Human Motion Prediction</h3>

        <h4>
			<a class="a-int" href="https://scholar.google.com/citations?user=fJQGPhEAAAAJ">Haodong Yan</a>, 
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>, <a class="a-int" href="https://www.imsb.uni-stuttgart.de/team/Schmitt-00006/">Syn Schmitt</a>, <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
        </h4>

        <h4>        
			<span class="pub_additional_journal">Proceedings of the Pacific Conference on Computer Graphics and Applications (Pacific Graphics), 2024: 1-12.
			</span>
		</h4>        

		<hr>
		<img src="./yan24_gazemodiff/image/teaser.png" style="height:auto; width:1024px;" class="centerContent"><br>
		<i class="centerContent"></i>            
        <hr>
				
	<h4>Abstract</h4>
	Human motion prediction is important for many virtual and augmented reality (VR/AR) applications such as collision avoidance and realistic avatar generation. Existing methods have synthesised body motion only from observed past motion, despite the fact that human eye gaze is known to correlate strongly with body movements and is readily available in recent VR/AR headsets. We present GazeMoDiff â€“ a novel gaze-guided denoising diffusion model to generate stochastic human motions. Our method first uses a gaze encoder and a motion encoder to extract the gaze and motion features respectively, then employs a graph attention network to fuse these features, and finally injects the gaze-motion features into a noise prediction network via a cross-attention mechanism to progressively generate multiple reasonable human motions in the future. Extensive experiments on the MoGaze and GIMO datasets demonstrate that our method outperforms the state-of-the-art methods by a large margin in terms of multi-modal final displacement error (17.3% on MoGaze and 13.3% on GIMO). We further conducted a human study (N=21) and validated that the motions generated by our method were perceived as both more precise and more realistic than those of prior methods. Taken together, these results reveal the significant information content available in eye gaze for stochastic human motion prediction as well as the effectiveness of our method in exploiting this information.
	<hr>
	
        <!--<h4>Paper Video</h4>
        <object type='application/x-shockwave-flash' style='width:1024px; height:608px;' data='https://www.youtube.com/v/I-ecIvRqOCY'>
        <param name='movie' value='https://www.youtube.com/v/I-ecIvRqOCY'/>
        </object>        
        <hr>-->
		
        <h4>Links</h4>
        <div class="pub_links">
                        
	        <svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./yan24_gazemodiff/pdf/yan24_gazemodiff.pdf">paper.pdf</a></p>
			<i class="fa fa-code"></i>&nbsp;Code: <a class="a-text-ext" href="https://github.com/yhd-ai/Gazemodiff">code</a></p>
        </div>
		
        <hr>

        <h4>BibTeX</h4>

<div class="pub_bibtex bg_grey">@inproceedings{yan24gazemodiff,
	title={GazeMoDiff: Gaze-guided Diffusion Model for Stochastic Human Motion Prediction},
	author={Yan, Haodong and Hu, Zhiming and Schmitt, Syn and Bulling, Andreas},
	booktitle={Proceedings of the Pacific Conference on Computer Graphics and Applications},	
	year={2024},
	pages={1--12},
	doi={10.2312/pg.20241315}}
</div>
    </div>
</div>

        </div>

        <!-- footer -->
        <div id="footer" class="footer-v1">
            
            <div class="copyright custom-copyright">
                <div class="container">
                    <div class="row">
                        <div class="col-md-6">
                            <p>
                                <span class="custom-copyright-container">
                                    Last modified: 28/10/2024
                                </span>
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        
    </body>
</html>