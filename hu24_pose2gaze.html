<!DOCTYPE html>
<html lang="en">
    <meta http-equiv="content-type" content="text/html;charset=utf-8" />
    <head>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
        <title>Pose2Gaze: Eye-body Coordination during Daily Activities for Gaze Prediction from Full-body Poses</title>
        <link rel="stylesheet" media="all" href="./index/css/main_v2.css"/>
        <link rel="stylesheet" media="all" href="./hu24_pose2gaze/css/owl.carousel.min.css" />
        <link rel="stylesheet" media="all" href="./hu24_pose2gaze/css/owl.theme.default.min.css" />
        <link rel="stylesheet" media="all" href="./hu24_pose2gaze/css/jquery-ui.min.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
        <script src="./hu24_pose2gaze/js/jquery-3.4.1.min.js"></script>
        <script src="./hu24_pose2gaze/js/jquery-ui.min.js"></script>
        <script src="./hu24_pose2gaze/js/fontawesome-5.11.2.js"></script>
        <script src="./hu24_pose2gaze/js/main.js"></script>
        <script src="./hu24_pose2gaze/js/owl.carousel.min.js"></script>
        <script src="./hu24_pose2gaze/js/rot13.js"></script>
		<script src="https://www.w3counter.com/tracker.js?id=150008"></script>
    </head>
    <body class="header header-location">
        <div class="wrapper">
            <!-- header -->
            <div id="unstickyheader" class="unstickyheader">
                <div class="topbar">
                    <div class="container" >
                        
                    </div>
                </div>
            </div>
        </div>
        

        <!-- content -->
        <div class="content">
        <div class="section margin-top-20 margin-bottom-40">
    <div class="container">

        <h3>Pose2Gaze: Eye-body Coordination during Daily Activities for Gaze Prediction from Full-body Poses</h3>

        <h4>
        
            <a class="a-int" href="https://cranehzm.github.io/">Zhiming Hu</a>, Jiahui Xu, <a class="a-int" href="https://www.imsb.uni-stuttgart.de/team/Schmitt-00006/">Syn Schmitt</a>, <a class="a-int" href="https://www.perceptualui.org/people/bulling/">Andreas Bulling</a>
        </h4>

        <h4>
        <span class="pub_additional_journal">IEEE Transactions on Visualization and Computer Graphics (TVCG, oral presentation at ISMAR 2024), 2025, 31(9): 4655-4666.
		</span>		
        </h4>
		
		<hr>
		<img src="./hu24_pose2gaze/image/teaser.png" style="height:auto; width:1024px;" class="centerContent"><br>
		<i class="centerContent"></i>            
        <hr>
		
	<h4>Abstract</h4>
	Human eye gaze plays a significant role in many virtual and augmented reality (VR/AR) applications, such as gaze-contingent rendering, gaze-based interaction, or eye-based activity recognition. However, prior works on gaze analysis and prediction have only explored eye-head coordination and were limited to human-object interactions. We first report a comprehensive analysis of eye-body coordination in various human-object and human-human interaction activities based on four public datasets collected in real-world (MoGaze), VR (ADT), as well as AR (GIMO and EgoBody) environments. We show that in human-object interactions, e.g. pick and place, eye gaze exhibits strong correlations with full-body motion while in human-human interactions, e.g. chat and teach, a person’s gaze direction is correlated with the body orientation towards the interaction partner. Informed by these analyses we then present Pose2Gaze – a novel eye-body coordination model that uses a convolutional neural network and a spatio-temporal graph convolutional neural network to extract features from head direction and full-body poses, respectively, and then uses a convolutional neural network to predict eye gaze. We compare our method with state-of-the-art methods that predict eye gaze only from head movements and show that Pose2Gaze outperforms these baselines with an average improvement of 24.0% on MoGaze, 10.1% on ADT, 21.3% on GIMO, and 28.6% on EgoBody in mean angular error, respectively. We also show that our method significantly outperforms prior methods in the sample downstream task of eye-based activity recognition. These results underline the significant information content available in eye-body coordination during daily activities and open up a new direction for gaze prediction.
	<hr>

	<h4>Presentation Video</h4>
	<object type='application/x-shockwave-flash' style='width:1024px; height:576px;' data='https://www.youtube.com/v/NsUic85mAmY' class="centerContent">
	<param name='movie' value='https://www.youtube.com/v/NsUic85mAmY'/>
	</object>        
	<br>
	
	<h4>Demo Video</h4>
	<object type='application/x-shockwave-flash' style='width:1024px; height:576px;' data='https://www.youtube.com/v/ell7__dMu0I' class="centerContent">
	<param name='movie' value='https://www.youtube.com/v/ell7__dMu0I'/>
	</object>        
	<hr>


	<h4>Links</h4>
	<div class="pub_links">
					
		<svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fa" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg><p>Paper: <a class="pub_list" href="./hu24_pose2gaze/pdf/hu24_pose2gaze.pdf">paper.pdf</a></p>
		<i class="fa fa-file-powerpoint"></i>&nbsp; Slides: <a class="pub_list" href="./hu24_pose2gaze/ppt/hu24_pose2gaze.pdf">slides.pdf</a></p>
		<i class="fa fa-code"></i>&nbsp;Code: <a class="a-text-ext" href="https://github.com/CraneHzm/Pose2Gaze">code</a></p>
	</div>
		
	<hr>

	<h4>BibTeX</h4>

<div class="pub_bibtex bg_grey">@article{hu24pose2gaze,
	author={Hu, Zhiming and Xu, Jiahui and Schmitt, Syn and Bulling, Andreas},
	journal={IEEE Transactions on Visualization and Computer Graphics}, 
	title={Pose2Gaze: Eye-body Coordination during Daily Activities for Gaze Prediction from Full-body Poses}, 
	year={2025},
	volume={31},
	number={9},
	pages={4655--4666},
	doi={10.1109/TVCG.2024.3412190}} 
</div>
    </div>
</div>

        </div>

        <!-- footer -->
        <div id="footer" class="footer-v1">
            
            <div class="copyright custom-copyright">
                <div class="container">
                    <div class="row">
                        <div class="col-md-6">
                            <p>
                                <span class="custom-copyright-container">
                                    Last modified: 13/08/2025
                                </span>
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        
    </body>
</html>